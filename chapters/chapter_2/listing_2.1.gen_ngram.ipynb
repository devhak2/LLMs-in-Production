{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# N-gram 언어 모델 생성하기\n",
        "\n",
        "이 노트북에서는 NLTK를 사용하여 N-gram 언어 모델을 만드는 방법을 배워보겠습니다.\n",
        "햄릿 텍스트를 사용하여 3-gram 모델을 훈련하고, 텍스트를 생성하고, 확률을 계산해보겠습니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. 필요한 라이브러리 import\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from nltk.corpus.reader import PlaintextCorpusReader\n",
        "from nltk.util import everygrams\n",
        "from nltk.lm.preprocessing import (\n",
        "    pad_both_ends,\n",
        "    flatten,\n",
        "    padded_everygram_pipeline,\n",
        ")\n",
        "from nltk.lm import MLE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. NLTK 데이터 다운로드\n",
        "\n",
        "NLTK의 punkt 토크나이저가 필요합니다. 없으면 자동으로 다운로드합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NLTK punkt 토크나이저를 다운로드합니다...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\younghl\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    import nltk\n",
        "    nltk.data.find(\"tokenizers/punkt.zip\")\n",
        "    print(\"NLTK punkt 토크나이저가 이미 설치되어 있습니다.\")\n",
        "except LookupError:\n",
        "    print(\"NLTK punkt 토크나이저를 다운로드합니다...\")\n",
        "    nltk.download(\"punkt\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. 코퍼스 생성 및 데이터 확인\n",
        "\n",
        "햄릿 텍스트 파일을 읽어서 코퍼스를 만들고 문장들을 확인해보겠습니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "전체 문장 개수: 2660\n",
            "첫 번째 문장: ['The', 'Project', 'Gutenberg', 'EBook', 'of', 'Hamlet', ',', 'by', 'William', 'Shakespeare']\n",
            "두 번째 문장: ['This', 'eBook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restrictions', 'whatsoever', '.']\n"
          ]
        }
      ],
      "source": [
        "# 현재 디렉토리에서 .txt 파일들로 코퍼스 생성\n",
        "my_corpus = PlaintextCorpusReader(\"../../\", \".*\\.txt\")\n",
        "file_ids = \"./data/hamlet.txt\"\n",
        "\n",
        "# 전체 문장 개수 확인\n",
        "sentences = list(my_corpus.sents(fileids=file_ids))\n",
        "print(f\"전체 문장 개수: {len(sentences)}\")\n",
        "print(f\"첫 번째 문장: {sentences[0]}\")\n",
        "print(f\"두 번째 문장: {sentences[1]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. 몇 가지 문장 출력해보기\n",
        "\n",
        "처음 10개 문장을 확인해보겠습니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "문장 1: ['The', 'Project', 'Gutenberg', 'EBook', 'of', 'Hamlet', ',', 'by', 'William', 'Shakespeare']\n",
            "문장 2: ['This', 'eBook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restrictions', 'whatsoever', '.']\n",
            "문장 3: ['You', 'may', 'copy', 'it', ',', 'give', 'it', 'away', 'or', 're', '-', 'use', 'it', 'under', 'the', 'terms', 'of', 'the', 'Project', 'Gutenberg', 'License', 'included', 'with', 'this', 'eBook', 'or', 'online', 'at', 'www', '.', 'gutenberg', '.', 'org']\n",
            "문장 4: ['Title', ':', 'Hamlet']\n",
            "문장 5: ['Author', ':', 'William', 'Shakespeare']\n",
            "문장 6: ['Editor', ':', 'Charles', 'Kean']\n",
            "문장 7: ['Release', 'Date', ':', 'January', '10', ',', '2009', '[', 'EBook', '#', '27761', ']']\n",
            "문장 8: ['Language', ':', 'English']\n",
            "문장 9: ['Character', 'set', 'encoding', ':', 'UTF', '-', '8']\n",
            "문장 10: ['***', 'START', 'OF', 'THIS', 'PROJECT', 'GUTENBERG', 'EBOOK', 'HAMLET', '***']\n"
          ]
        }
      ],
      "source": [
        "# 처음 10개 문장만 출력\n",
        "for i, sent in enumerate(my_corpus.sents(fileids=file_ids)):\n",
        "    if i >= 10:\n",
        "        break\n",
        "    print(f\"문장 {i+1}: {sent}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. 패딩과 Everygrams 이해하기\n",
        "\n",
        "N-gram 모델에서는 문장의 시작과 끝을 표시하기 위해 패딩을 사용합니다.\n",
        "특정 문장(1104번째)을 예시로 패딩과 everygrams이 어떻게 작동하는지 보겠습니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "원본 문장: ['_Ham', '.', '_', 'To', 'be', ',', 'or', 'not', 'to', 'be', ',', 'that', 'is', 'the', 'question', ':[', '8', ']', 'Whether', \"'\", 'tis', 'nobler', 'in', 'the', 'mind', 'to', 'suffer', 'The', 'slings', 'and', 'arrows', 'of', 'outrageous', 'fortune', ',', 'Or', 'to', 'take', 'arms', 'against', 'a', 'sea', 'of', 'troubles', ',[', '9', ']', 'And', ',', 'by', 'opposing', 'end', 'them', '?--', 'To', 'die', ',--', 'to', 'sleep', ',', 'No', 'more', ';--', 'and', 'by', 'a', 'sleep', ',', 'to', 'say', 'we', 'end', 'The', 'heart', '-', 'ache', ',', 'and', 'the', 'thousand', 'natural', 'shocks', 'That', 'flesh', 'is', 'heir', 'to', ':', \"'\", 'tis', 'a', 'consummation', 'Devoutly', 'to', 'be', 'wished', '.']\n",
            "패딩된 문장: ['<s>', '_Ham', '.', '_', 'To', 'be', ',', 'or', 'not', 'to', 'be', ',', 'that', 'is', 'the', 'question', ':[', '8', ']', 'Whether', \"'\", 'tis', 'nobler', 'in', 'the', 'mind', 'to', 'suffer', 'The', 'slings', 'and', 'arrows', 'of', 'outrageous', 'fortune', ',', 'Or', 'to', 'take', 'arms', 'against', 'a', 'sea', 'of', 'troubles', ',[', '9', ']', 'And', ',', 'by', 'opposing', 'end', 'them', '?--', 'To', 'die', ',--', 'to', 'sleep', ',', 'No', 'more', ';--', 'and', 'by', 'a', 'sleep', ',', 'to', 'say', 'we', 'end', 'The', 'heart', '-', 'ache', ',', 'and', 'the', 'thousand', 'natural', 'shocks', 'That', 'flesh', 'is', 'heir', 'to', ':', \"'\", 'tis', 'a', 'consummation', 'Devoutly', 'to', 'be', 'wished', '.', '</s>']\n"
          ]
        }
      ],
      "source": [
        "# 1104번째 문장 확인\n",
        "example_sentence = my_corpus.sents(fileids=file_ids)[1104]\n",
        "print(f\"원본 문장: {example_sentence}\")\n",
        "\n",
        "# 패딩 적용 (n=2는 trigram을 위해 양쪽에 2개씩 패딩)\n",
        "padded_trigrams = list(pad_both_ends(example_sentence, n=2))\n",
        "print(f\"패딩된 문장: {padded_trigrams}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "생성된 everygrams 개수: 294\n",
            "\n",
            "처음 10개 everygrams:\n",
            "1: ('<s>',)\n",
            "2: ('<s>', '_Ham')\n",
            "3: ('<s>', '_Ham', '.')\n",
            "4: ('_Ham',)\n",
            "5: ('_Ham', '.')\n",
            "6: ('_Ham', '.', '_')\n",
            "7: ('.',)\n",
            "8: ('.', '_')\n",
            "9: ('.', '_', 'To')\n",
            "10: ('_',)\n"
          ]
        }
      ],
      "source": [
        "# Everygrams 생성 (1-gram부터 3-gram까지)\n",
        "everygrams_list = list(everygrams(padded_trigrams, max_len=3))\n",
        "print(f\"생성된 everygrams 개수: {len(everygrams_list)}\")\n",
        "print(\"\\n처음 10개 everygrams:\")\n",
        "for i, gram in enumerate(everygrams_list[:10]):\n",
        "    print(f\"{i+1}: {gram}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. 전체 코퍼스에 패딩 적용\n",
        "\n",
        "모든 문장에 패딩을 적용하고 평평하게 만들어봅시다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "평평하게 만든 코퍼스의 토큰 개수: 62000\n",
            "처음 20개 토큰: ['<s>', '<s>', 'The', 'Project', 'Gutenberg', 'EBook', 'of', 'Hamlet', ',', 'by', 'William', 'Shakespeare', '</s>', '</s>', '<s>', '<s>', 'This', 'eBook', 'is', 'for']\n"
          ]
        }
      ],
      "source": [
        "# 전체 코퍼스에 패딩 적용\n",
        "flattened_corpus = list(\n",
        "    flatten(\n",
        "        pad_both_ends(sent, n=3)\n",
        "        for sent in my_corpus.sents(fileids=file_ids)\n",
        "    )\n",
        ")\n",
        "\n",
        "print(f\"평평하게 만든 코퍼스의 토큰 개수: {len(flattened_corpus)}\")\n",
        "print(f\"처음 20개 토큰: {flattened_corpus[:20]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. 훈련 데이터와 어휘집 생성\n",
        "\n",
        "`padded_everygram_pipeline`을 사용하여 훈련 데이터와 어휘집을 한 번에 생성합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "훈련 데이터와 어휘집이 생성되었습니다.\n"
          ]
        }
      ],
      "source": [
        "# 3-gram 모델을 위한 훈련 데이터와 어휘집 생성\n",
        "train, vocab = padded_everygram_pipeline(\n",
        "    3, my_corpus.sents(fileids=file_ids)\n",
        ")\n",
        "\n",
        "print(\"훈련 데이터와 어휘집이 생성되었습니다.\")\n",
        "# print(f\"훈련 데이터 크기: {len(list(train))}\")\n",
        "# print(f\"전체 문장 갯수: {len(my_corpus.sents(fileids=file_ids))}\")\n",
        "# print(f\"어휘집 크기: {len(list(vocab))}\")\n",
        "# print(f\"전체 토큰 갯수: {len(flattened_corpus)}\")\n",
        "# print(f\"어휘집 샘플: {list(vocab)[:30]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. MLE 모델 인스턴스 생성\n",
        "\n",
        "Maximum Likelihood Estimator를 사용하여 3-gram 모델을 생성합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "훈련 전 어휘집 크기: 0\n",
            "모델 order: 3\n"
          ]
        }
      ],
      "source": [
        "# 3-gram MLE 모델 생성\n",
        "lm = MLE(3)\n",
        "print(f\"훈련 전 어휘집 크기: {len(lm.vocab)}\")\n",
        "print(f\"모델 order: {lm.order}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. 모델 훈련\n",
        "\n",
        "생성된 훈련 데이터와 어휘집으로 모델을 훈련합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "모델 훈련이 완료되었습니다!\n",
            "훈련 후 어휘집 크기: 6738\n",
            "어휘집 샘플: ['<s>', 'The', 'Project', 'Gutenberg', 'EBook', 'of', 'Hamlet', ',', 'by', 'William', 'Shakespeare', '</s>', 'This', 'eBook', 'is', 'for', 'the', 'use', 'anyone', 'anywhere']\n"
          ]
        }
      ],
      "source": [
        "# 모델 훈련\n",
        "lm.fit(train, vocab)\n",
        "print(\"모델 훈련이 완료되었습니다!\")\n",
        "print(f\"훈련 후 어휘집 크기: {len(lm.vocab)}\")\n",
        "print(f\"어휘집 샘플: {list(lm.vocab)[:20]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. 텍스트 생성\n",
        "\n",
        "훈련된 모델로 텍스트를 생성해보겠습니다. 'to be'로 시작하는 6개 단어를 생성합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "생성된 텍스트: one of their deities , and\n",
            "\n",
            "다른 생성 결과들:\n",
            "1: heard ,[ 35 ] These but\n",
            "2: one man picked out of tune\n",
            "3: buried in ' t ? </s>\n"
          ]
        }
      ],
      "source": [
        "# 'to be'로 시작하는 6개 단어 생성\n",
        "generated_text = lm.generate(6, [\"to\", \"be\"])\n",
        "print(f\"생성된 텍스트: {' '.join(generated_text)}\")\n",
        "\n",
        "# 여러 번 생성해보기\n",
        "print(\"\\n다른 생성 결과들:\")\n",
        "for i in range(3):\n",
        "    generated = lm.generate(6, [\"to\", \"be\"])\n",
        "    print(f\"{i+1}: {' '.join(generated)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. 어휘집 lookup 테스트\n",
        "\n",
        "어휘집에서 단어들을 찾아보고 OOV(Out-of-Vocabulary) 처리를 확인합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "원본 문장: ['_Ham', '.', '_', 'To', 'be', ',', 'or', 'not', 'to', 'be', ',', 'that', 'is', 'the', 'question', ':[', '8', ']', 'Whether', \"'\", 'tis', 'nobler', 'in', 'the', 'mind', 'to', 'suffer', 'The', 'slings', 'and', 'arrows', 'of', 'outrageous', 'fortune', ',', 'Or', 'to', 'take', 'arms', 'against', 'a', 'sea', 'of', 'troubles', ',[', '9', ']', 'And', ',', 'by', 'opposing', 'end', 'them', '?--', 'To', 'die', ',--', 'to', 'sleep', ',', 'No', 'more', ';--', 'and', 'by', 'a', 'sleep', ',', 'to', 'say', 'we', 'end', 'The', 'heart', '-', 'ache', ',', 'and', 'the', 'thousand', 'natural', 'shocks', 'That', 'flesh', 'is', 'heir', 'to', ':', \"'\", 'tis', 'a', 'consummation', 'Devoutly', 'to', 'be', 'wished', '.']\n",
            "Lookup 결과: ('_Ham', '.', '_', 'To', 'be', ',', 'or', 'not', 'to', 'be', ',', 'that', 'is', 'the', 'question', ':[', '8', ']', 'Whether', \"'\", 'tis', 'nobler', 'in', 'the', 'mind', 'to', 'suffer', 'The', 'slings', 'and', 'arrows', 'of', 'outrageous', 'fortune', ',', 'Or', 'to', 'take', 'arms', 'against', 'a', 'sea', 'of', 'troubles', ',[', '9', ']', 'And', ',', 'by', 'opposing', 'end', 'them', '?--', 'To', 'die', ',--', 'to', 'sleep', ',', 'No', 'more', ';--', 'and', 'by', 'a', 'sleep', ',', 'to', 'say', 'we', 'end', 'The', 'heart', '-', 'ache', ',', 'and', 'the', 'thousand', 'natural', 'shocks', 'That', 'flesh', 'is', 'heir', 'to', ':', \"'\", 'tis', 'a', 'consummation', 'Devoutly', 'to', 'be', 'wished', '.')\n",
            "\n",
            "OOV 테스트 단어들: ['aliens', 'from', 'Mars']\n",
            "OOV Lookup 결과: ('<UNK>', 'from', 'Mars')\n",
            "<UNK> 토큰이 사용되었습니다: True\n"
          ]
        }
      ],
      "source": [
        "# 실제 문장에서 어휘집 lookup\n",
        "test_sentence = my_corpus.sents(fileids=file_ids)[1104]\n",
        "lookup_result = lm.vocab.lookup(test_sentence)\n",
        "print(f\"원본 문장: {test_sentence}\")\n",
        "print(f\"Lookup 결과: {lookup_result}\")\n",
        "\n",
        "# OOV 단어들 테스트\n",
        "oov_words = [\"aliens\", \"from\", \"Mars\"]\n",
        "oov_result = lm.vocab.lookup(oov_words)\n",
        "print(f\"\\nOOV 테스트 단어들: {oov_words}\")\n",
        "print(f\"OOV Lookup 결과: {oov_result}\")\n",
        "print(f\"<UNK> 토큰이 사용되었습니다: {'<UNK>' in oov_result}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. N-gram 빈도수 확인\n",
        "\n",
        "모델에서 특정 N-gram의 빈도수를 확인해보겠습니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "카운트 객체 타입: <class 'nltk.lm.counter.NgramCounter'>\n",
            "'to be'의 빈도수: 43\n",
            "'to'의 전체 빈도수: 754\n"
          ]
        }
      ],
      "source": [
        "# 전체 카운트 정보\n",
        "print(f\"카운트 객체 타입: {type(lm.counts)}\")\n",
        "\n",
        "# 'to be'의 빈도수\n",
        "to_be_count = lm.counts[[\"to\"]][\"be\"]\n",
        "print(f\"'to be'의 빈도수: {to_be_count}\")\n",
        "\n",
        "# 'to'의 전체 빈도수\n",
        "to_count = lm.counts['to']\n",
        "print(f\"'to'의 전체 빈도수: {to_count}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. 확률 계산\n",
        "\n",
        "단어의 확률을 계산해보겠습니다. 조건부 확률도 확인해보겠습니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "P(be) = 0.003210\n",
            "P(be|to) = 0.057029\n",
            "P(be|not, to) = 0.272727\n"
          ]
        }
      ],
      "source": [
        "# 단순 확률\n",
        "be_prob = lm.score(\"be\")\n",
        "print(f\"P(be) = {be_prob:.6f}\")\n",
        "\n",
        "# 조건부 확률: P(be|to)\n",
        "be_given_to = lm.score(\"be\", [\"to\"])\n",
        "print(f\"P(be|to) = {be_given_to:.6f}\")\n",
        "\n",
        "# 조건부 확률: P(be|not, to)\n",
        "be_given_not_to = lm.score(\"be\", [\"not\", \"to\"])\n",
        "print(f\"P(be|not, to) = {be_given_not_to:.6f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 14. 로그 확률 계산\n",
        "\n",
        "매우 작은 확률값을 다루기 위해 로그 스케일로 확률을 계산합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "log P(be) = -8.283356\n",
            "log P(be|to) = -4.132156\n",
            "log P(be|not, to) = -1.874469\n"
          ]
        }
      ],
      "source": [
        "# 로그 확률\n",
        "be_logprob = lm.logscore(\"be\")\n",
        "print(f\"log P(be) = {be_logprob:.6f}\")\n",
        "\n",
        "# 조건부 로그 확률\n",
        "be_given_to_log = lm.logscore(\"be\", [\"to\"])\n",
        "print(f\"log P(be|to) = {be_given_to_log:.6f}\")\n",
        "\n",
        "be_given_not_to_log = lm.logscore(\"be\", [\"not\", \"to\"])\n",
        "print(f\"log P(be|not, to) = {be_given_not_to_log:.6f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 15. 엔트로피와 퍼플렉서티 계산\n",
        "\n",
        "모델의 성능을 평가하기 위해 엔트로피와 퍼플렉서티를 계산합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "테스트 데이터: [('to', 'be'), ('or', 'not'), ('to', 'be')]\n",
            "엔트로피: 4.995137\n",
            "퍼플렉서티: 31.892318\n",
            "\n",
            "💡 참고: 낮은 퍼플렉서티는 더 좋은 모델을 의미합니다!\n"
          ]
        }
      ],
      "source": [
        "# 테스트 데이터\n",
        "test = [(\"to\", \"be\"), (\"or\", \"not\"), (\"to\", \"be\")]\n",
        "print(f\"테스트 데이터: {test}\")\n",
        "\n",
        "# 엔트로피 계산\n",
        "entropy = lm.entropy(test)\n",
        "print(f\"엔트로피: {entropy:.6f}\")\n",
        "\n",
        "# 퍼플렉서티 계산\n",
        "perplexity = lm.perplexity(test)\n",
        "print(f\"퍼플렉서티: {perplexity:.6f}\")\n",
        "\n",
        "print(\"\\n💡 참고: 낮은 퍼플렉서티는 더 좋은 모델을 의미합니다!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 결론\n",
        "\n",
        "이 노트북에서 우리는:\n",
        "1. NLTK를 사용하여 텍스트 코퍼스를 처리했습니다\n",
        "2. 패딩과 N-gram 생성을 배웠습니다\n",
        "3. Maximum Likelihood Estimator로 3-gram 모델을 훈련했습니다\n",
        "4. 텍스트 생성을 수행했습니다\n",
        "5. 확률 계산과 모델 평가 방법을 배웠습니다\n",
        "\n",
        "N-gram 모델은 간단하지만 언어 모델링의 기초를 이해하는 데 매우 유용합니다!\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
