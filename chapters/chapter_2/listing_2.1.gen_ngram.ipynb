{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# N-gram ì–¸ì–´ ëª¨ë¸ ìƒì„±í•˜ê¸°\n",
        "\n",
        "ì´ ë…¸íŠ¸ë¶ì—ì„œëŠ” NLTKë¥¼ ì‚¬ìš©í•˜ì—¬ N-gram ì–¸ì–´ ëª¨ë¸ì„ ë§Œë“œëŠ” ë°©ë²•ì„ ë°°ì›Œë³´ê² ìŠµë‹ˆë‹¤.\n",
        "í–„ë¦¿ í…ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ 3-gram ëª¨ë¸ì„ í›ˆë ¨í•˜ê³ , í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ê³ , í™•ë¥ ì„ ê³„ì‚°í•´ë³´ê² ìŠµë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ import\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from nltk.corpus.reader import PlaintextCorpusReader\n",
        "from nltk.util import everygrams\n",
        "from nltk.lm.preprocessing import (\n",
        "    pad_both_ends,\n",
        "    flatten,\n",
        "    padded_everygram_pipeline,\n",
        ")\n",
        "from nltk.lm import MLE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. NLTK ë°ì´í„° ë‹¤ìš´ë¡œë“œ\n",
        "\n",
        "NLTKì˜ punkt í† í¬ë‚˜ì´ì €ê°€ í•„ìš”í•©ë‹ˆë‹¤. ì—†ìœ¼ë©´ ìë™ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NLTK punkt í† í¬ë‚˜ì´ì €ë¥¼ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\younghl\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    import nltk\n",
        "    nltk.data.find(\"tokenizers/punkt.zip\")\n",
        "    print(\"NLTK punkt í† í¬ë‚˜ì´ì €ê°€ ì´ë¯¸ ì„¤ì¹˜ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\")\n",
        "except LookupError:\n",
        "    print(\"NLTK punkt í† í¬ë‚˜ì´ì €ë¥¼ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤...\")\n",
        "    nltk.download(\"punkt\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. ì½”í¼ìŠ¤ ìƒì„± ë° ë°ì´í„° í™•ì¸\n",
        "\n",
        "í–„ë¦¿ í…ìŠ¤íŠ¸ íŒŒì¼ì„ ì½ì–´ì„œ ì½”í¼ìŠ¤ë¥¼ ë§Œë“¤ê³  ë¬¸ì¥ë“¤ì„ í™•ì¸í•´ë³´ê² ìŠµë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ì „ì²´ ë¬¸ì¥ ê°œìˆ˜: 2660\n",
            "ì²« ë²ˆì§¸ ë¬¸ì¥: ['The', 'Project', 'Gutenberg', 'EBook', 'of', 'Hamlet', ',', 'by', 'William', 'Shakespeare']\n",
            "ë‘ ë²ˆì§¸ ë¬¸ì¥: ['This', 'eBook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restrictions', 'whatsoever', '.']\n"
          ]
        }
      ],
      "source": [
        "# í˜„ì¬ ë””ë ‰í† ë¦¬ì—ì„œ .txt íŒŒì¼ë“¤ë¡œ ì½”í¼ìŠ¤ ìƒì„±\n",
        "my_corpus = PlaintextCorpusReader(\"../../\", \".*\\.txt\")\n",
        "file_ids = \"./data/hamlet.txt\"\n",
        "\n",
        "# ì „ì²´ ë¬¸ì¥ ê°œìˆ˜ í™•ì¸\n",
        "sentences = list(my_corpus.sents(fileids=file_ids))\n",
        "print(f\"ì „ì²´ ë¬¸ì¥ ê°œìˆ˜: {len(sentences)}\")\n",
        "print(f\"ì²« ë²ˆì§¸ ë¬¸ì¥: {sentences[0]}\")\n",
        "print(f\"ë‘ ë²ˆì§¸ ë¬¸ì¥: {sentences[1]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. ëª‡ ê°€ì§€ ë¬¸ì¥ ì¶œë ¥í•´ë³´ê¸°\n",
        "\n",
        "ì²˜ìŒ 10ê°œ ë¬¸ì¥ì„ í™•ì¸í•´ë³´ê² ìŠµë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ë¬¸ì¥ 1: ['The', 'Project', 'Gutenberg', 'EBook', 'of', 'Hamlet', ',', 'by', 'William', 'Shakespeare']\n",
            "ë¬¸ì¥ 2: ['This', 'eBook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restrictions', 'whatsoever', '.']\n",
            "ë¬¸ì¥ 3: ['You', 'may', 'copy', 'it', ',', 'give', 'it', 'away', 'or', 're', '-', 'use', 'it', 'under', 'the', 'terms', 'of', 'the', 'Project', 'Gutenberg', 'License', 'included', 'with', 'this', 'eBook', 'or', 'online', 'at', 'www', '.', 'gutenberg', '.', 'org']\n",
            "ë¬¸ì¥ 4: ['Title', ':', 'Hamlet']\n",
            "ë¬¸ì¥ 5: ['Author', ':', 'William', 'Shakespeare']\n",
            "ë¬¸ì¥ 6: ['Editor', ':', 'Charles', 'Kean']\n",
            "ë¬¸ì¥ 7: ['Release', 'Date', ':', 'January', '10', ',', '2009', '[', 'EBook', '#', '27761', ']']\n",
            "ë¬¸ì¥ 8: ['Language', ':', 'English']\n",
            "ë¬¸ì¥ 9: ['Character', 'set', 'encoding', ':', 'UTF', '-', '8']\n",
            "ë¬¸ì¥ 10: ['***', 'START', 'OF', 'THIS', 'PROJECT', 'GUTENBERG', 'EBOOK', 'HAMLET', '***']\n"
          ]
        }
      ],
      "source": [
        "# ì²˜ìŒ 10ê°œ ë¬¸ì¥ë§Œ ì¶œë ¥\n",
        "for i, sent in enumerate(my_corpus.sents(fileids=file_ids)):\n",
        "    if i >= 10:\n",
        "        break\n",
        "    print(f\"ë¬¸ì¥ {i+1}: {sent}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. íŒ¨ë”©ê³¼ Everygrams ì´í•´í•˜ê¸°\n",
        "\n",
        "N-gram ëª¨ë¸ì—ì„œëŠ” ë¬¸ì¥ì˜ ì‹œì‘ê³¼ ëì„ í‘œì‹œí•˜ê¸° ìœ„í•´ íŒ¨ë”©ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
        "íŠ¹ì • ë¬¸ì¥(1104ë²ˆì§¸)ì„ ì˜ˆì‹œë¡œ íŒ¨ë”©ê³¼ everygramsì´ ì–´ë–»ê²Œ ì‘ë™í•˜ëŠ”ì§€ ë³´ê² ìŠµë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ì›ë³¸ ë¬¸ì¥: ['_Ham', '.', '_', 'To', 'be', ',', 'or', 'not', 'to', 'be', ',', 'that', 'is', 'the', 'question', ':[', '8', ']', 'Whether', \"'\", 'tis', 'nobler', 'in', 'the', 'mind', 'to', 'suffer', 'The', 'slings', 'and', 'arrows', 'of', 'outrageous', 'fortune', ',', 'Or', 'to', 'take', 'arms', 'against', 'a', 'sea', 'of', 'troubles', ',[', '9', ']', 'And', ',', 'by', 'opposing', 'end', 'them', '?--', 'To', 'die', ',--', 'to', 'sleep', ',', 'No', 'more', ';--', 'and', 'by', 'a', 'sleep', ',', 'to', 'say', 'we', 'end', 'The', 'heart', '-', 'ache', ',', 'and', 'the', 'thousand', 'natural', 'shocks', 'That', 'flesh', 'is', 'heir', 'to', ':', \"'\", 'tis', 'a', 'consummation', 'Devoutly', 'to', 'be', 'wished', '.']\n",
            "íŒ¨ë”©ëœ ë¬¸ì¥: ['<s>', '_Ham', '.', '_', 'To', 'be', ',', 'or', 'not', 'to', 'be', ',', 'that', 'is', 'the', 'question', ':[', '8', ']', 'Whether', \"'\", 'tis', 'nobler', 'in', 'the', 'mind', 'to', 'suffer', 'The', 'slings', 'and', 'arrows', 'of', 'outrageous', 'fortune', ',', 'Or', 'to', 'take', 'arms', 'against', 'a', 'sea', 'of', 'troubles', ',[', '9', ']', 'And', ',', 'by', 'opposing', 'end', 'them', '?--', 'To', 'die', ',--', 'to', 'sleep', ',', 'No', 'more', ';--', 'and', 'by', 'a', 'sleep', ',', 'to', 'say', 'we', 'end', 'The', 'heart', '-', 'ache', ',', 'and', 'the', 'thousand', 'natural', 'shocks', 'That', 'flesh', 'is', 'heir', 'to', ':', \"'\", 'tis', 'a', 'consummation', 'Devoutly', 'to', 'be', 'wished', '.', '</s>']\n"
          ]
        }
      ],
      "source": [
        "# 1104ë²ˆì§¸ ë¬¸ì¥ í™•ì¸\n",
        "example_sentence = my_corpus.sents(fileids=file_ids)[1104]\n",
        "print(f\"ì›ë³¸ ë¬¸ì¥: {example_sentence}\")\n",
        "\n",
        "# íŒ¨ë”© ì ìš© (n=2ëŠ” trigramì„ ìœ„í•´ ì–‘ìª½ì— 2ê°œì”© íŒ¨ë”©)\n",
        "padded_trigrams = list(pad_both_ends(example_sentence, n=2))\n",
        "print(f\"íŒ¨ë”©ëœ ë¬¸ì¥: {padded_trigrams}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ìƒì„±ëœ everygrams ê°œìˆ˜: 294\n",
            "\n",
            "ì²˜ìŒ 10ê°œ everygrams:\n",
            "1: ('<s>',)\n",
            "2: ('<s>', '_Ham')\n",
            "3: ('<s>', '_Ham', '.')\n",
            "4: ('_Ham',)\n",
            "5: ('_Ham', '.')\n",
            "6: ('_Ham', '.', '_')\n",
            "7: ('.',)\n",
            "8: ('.', '_')\n",
            "9: ('.', '_', 'To')\n",
            "10: ('_',)\n"
          ]
        }
      ],
      "source": [
        "# Everygrams ìƒì„± (1-gramë¶€í„° 3-gramê¹Œì§€)\n",
        "everygrams_list = list(everygrams(padded_trigrams, max_len=3))\n",
        "print(f\"ìƒì„±ëœ everygrams ê°œìˆ˜: {len(everygrams_list)}\")\n",
        "print(\"\\nì²˜ìŒ 10ê°œ everygrams:\")\n",
        "for i, gram in enumerate(everygrams_list[:10]):\n",
        "    print(f\"{i+1}: {gram}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. ì „ì²´ ì½”í¼ìŠ¤ì— íŒ¨ë”© ì ìš©\n",
        "\n",
        "ëª¨ë“  ë¬¸ì¥ì— íŒ¨ë”©ì„ ì ìš©í•˜ê³  í‰í‰í•˜ê²Œ ë§Œë“¤ì–´ë´…ì‹œë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "í‰í‰í•˜ê²Œ ë§Œë“  ì½”í¼ìŠ¤ì˜ í† í° ê°œìˆ˜: 62000\n",
            "ì²˜ìŒ 20ê°œ í† í°: ['<s>', '<s>', 'The', 'Project', 'Gutenberg', 'EBook', 'of', 'Hamlet', ',', 'by', 'William', 'Shakespeare', '</s>', '</s>', '<s>', '<s>', 'This', 'eBook', 'is', 'for']\n"
          ]
        }
      ],
      "source": [
        "# ì „ì²´ ì½”í¼ìŠ¤ì— íŒ¨ë”© ì ìš©\n",
        "flattened_corpus = list(\n",
        "    flatten(\n",
        "        pad_both_ends(sent, n=3)\n",
        "        for sent in my_corpus.sents(fileids=file_ids)\n",
        "    )\n",
        ")\n",
        "\n",
        "print(f\"í‰í‰í•˜ê²Œ ë§Œë“  ì½”í¼ìŠ¤ì˜ í† í° ê°œìˆ˜: {len(flattened_corpus)}\")\n",
        "print(f\"ì²˜ìŒ 20ê°œ í† í°: {flattened_corpus[:20]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. í›ˆë ¨ ë°ì´í„°ì™€ ì–´íœ˜ì§‘ ìƒì„±\n",
        "\n",
        "`padded_everygram_pipeline`ì„ ì‚¬ìš©í•˜ì—¬ í›ˆë ¨ ë°ì´í„°ì™€ ì–´íœ˜ì§‘ì„ í•œ ë²ˆì— ìƒì„±í•©ë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "í›ˆë ¨ ë°ì´í„°ì™€ ì–´íœ˜ì§‘ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
          ]
        }
      ],
      "source": [
        "# 3-gram ëª¨ë¸ì„ ìœ„í•œ í›ˆë ¨ ë°ì´í„°ì™€ ì–´íœ˜ì§‘ ìƒì„±\n",
        "train, vocab = padded_everygram_pipeline(\n",
        "    3, my_corpus.sents(fileids=file_ids)\n",
        ")\n",
        "\n",
        "print(\"í›ˆë ¨ ë°ì´í„°ì™€ ì–´íœ˜ì§‘ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
        "# print(f\"í›ˆë ¨ ë°ì´í„° í¬ê¸°: {len(list(train))}\")\n",
        "# print(f\"ì „ì²´ ë¬¸ì¥ ê°¯ìˆ˜: {len(my_corpus.sents(fileids=file_ids))}\")\n",
        "# print(f\"ì–´íœ˜ì§‘ í¬ê¸°: {len(list(vocab))}\")\n",
        "# print(f\"ì „ì²´ í† í° ê°¯ìˆ˜: {len(flattened_corpus)}\")\n",
        "# print(f\"ì–´íœ˜ì§‘ ìƒ˜í”Œ: {list(vocab)[:30]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. MLE ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±\n",
        "\n",
        "Maximum Likelihood Estimatorë¥¼ ì‚¬ìš©í•˜ì—¬ 3-gram ëª¨ë¸ì„ ìƒì„±í•©ë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "í›ˆë ¨ ì „ ì–´íœ˜ì§‘ í¬ê¸°: 0\n",
            "ëª¨ë¸ order: 3\n"
          ]
        }
      ],
      "source": [
        "# 3-gram MLE ëª¨ë¸ ìƒì„±\n",
        "lm = MLE(3)\n",
        "print(f\"í›ˆë ¨ ì „ ì–´íœ˜ì§‘ í¬ê¸°: {len(lm.vocab)}\")\n",
        "print(f\"ëª¨ë¸ order: {lm.order}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. ëª¨ë¸ í›ˆë ¨\n",
        "\n",
        "ìƒì„±ëœ í›ˆë ¨ ë°ì´í„°ì™€ ì–´íœ˜ì§‘ìœ¼ë¡œ ëª¨ë¸ì„ í›ˆë ¨í•©ë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ëª¨ë¸ í›ˆë ¨ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!\n",
            "í›ˆë ¨ í›„ ì–´íœ˜ì§‘ í¬ê¸°: 6738\n",
            "ì–´íœ˜ì§‘ ìƒ˜í”Œ: ['<s>', 'The', 'Project', 'Gutenberg', 'EBook', 'of', 'Hamlet', ',', 'by', 'William', 'Shakespeare', '</s>', 'This', 'eBook', 'is', 'for', 'the', 'use', 'anyone', 'anywhere']\n"
          ]
        }
      ],
      "source": [
        "# ëª¨ë¸ í›ˆë ¨\n",
        "lm.fit(train, vocab)\n",
        "print(\"ëª¨ë¸ í›ˆë ¨ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!\")\n",
        "print(f\"í›ˆë ¨ í›„ ì–´íœ˜ì§‘ í¬ê¸°: {len(lm.vocab)}\")\n",
        "print(f\"ì–´íœ˜ì§‘ ìƒ˜í”Œ: {list(lm.vocab)[:20]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. í…ìŠ¤íŠ¸ ìƒì„±\n",
        "\n",
        "í›ˆë ¨ëœ ëª¨ë¸ë¡œ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•´ë³´ê² ìŠµë‹ˆë‹¤. 'to be'ë¡œ ì‹œì‘í•˜ëŠ” 6ê°œ ë‹¨ì–´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ìƒì„±ëœ í…ìŠ¤íŠ¸: one of their deities , and\n",
            "\n",
            "ë‹¤ë¥¸ ìƒì„± ê²°ê³¼ë“¤:\n",
            "1: heard ,[ 35 ] These but\n",
            "2: one man picked out of tune\n",
            "3: buried in ' t ? </s>\n"
          ]
        }
      ],
      "source": [
        "# 'to be'ë¡œ ì‹œì‘í•˜ëŠ” 6ê°œ ë‹¨ì–´ ìƒì„±\n",
        "generated_text = lm.generate(6, [\"to\", \"be\"])\n",
        "print(f\"ìƒì„±ëœ í…ìŠ¤íŠ¸: {' '.join(generated_text)}\")\n",
        "\n",
        "# ì—¬ëŸ¬ ë²ˆ ìƒì„±í•´ë³´ê¸°\n",
        "print(\"\\në‹¤ë¥¸ ìƒì„± ê²°ê³¼ë“¤:\")\n",
        "for i in range(3):\n",
        "    generated = lm.generate(6, [\"to\", \"be\"])\n",
        "    print(f\"{i+1}: {' '.join(generated)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. ì–´íœ˜ì§‘ lookup í…ŒìŠ¤íŠ¸\n",
        "\n",
        "ì–´íœ˜ì§‘ì—ì„œ ë‹¨ì–´ë“¤ì„ ì°¾ì•„ë³´ê³  OOV(Out-of-Vocabulary) ì²˜ë¦¬ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ì›ë³¸ ë¬¸ì¥: ['_Ham', '.', '_', 'To', 'be', ',', 'or', 'not', 'to', 'be', ',', 'that', 'is', 'the', 'question', ':[', '8', ']', 'Whether', \"'\", 'tis', 'nobler', 'in', 'the', 'mind', 'to', 'suffer', 'The', 'slings', 'and', 'arrows', 'of', 'outrageous', 'fortune', ',', 'Or', 'to', 'take', 'arms', 'against', 'a', 'sea', 'of', 'troubles', ',[', '9', ']', 'And', ',', 'by', 'opposing', 'end', 'them', '?--', 'To', 'die', ',--', 'to', 'sleep', ',', 'No', 'more', ';--', 'and', 'by', 'a', 'sleep', ',', 'to', 'say', 'we', 'end', 'The', 'heart', '-', 'ache', ',', 'and', 'the', 'thousand', 'natural', 'shocks', 'That', 'flesh', 'is', 'heir', 'to', ':', \"'\", 'tis', 'a', 'consummation', 'Devoutly', 'to', 'be', 'wished', '.']\n",
            "Lookup ê²°ê³¼: ('_Ham', '.', '_', 'To', 'be', ',', 'or', 'not', 'to', 'be', ',', 'that', 'is', 'the', 'question', ':[', '8', ']', 'Whether', \"'\", 'tis', 'nobler', 'in', 'the', 'mind', 'to', 'suffer', 'The', 'slings', 'and', 'arrows', 'of', 'outrageous', 'fortune', ',', 'Or', 'to', 'take', 'arms', 'against', 'a', 'sea', 'of', 'troubles', ',[', '9', ']', 'And', ',', 'by', 'opposing', 'end', 'them', '?--', 'To', 'die', ',--', 'to', 'sleep', ',', 'No', 'more', ';--', 'and', 'by', 'a', 'sleep', ',', 'to', 'say', 'we', 'end', 'The', 'heart', '-', 'ache', ',', 'and', 'the', 'thousand', 'natural', 'shocks', 'That', 'flesh', 'is', 'heir', 'to', ':', \"'\", 'tis', 'a', 'consummation', 'Devoutly', 'to', 'be', 'wished', '.')\n",
            "\n",
            "OOV í…ŒìŠ¤íŠ¸ ë‹¨ì–´ë“¤: ['aliens', 'from', 'Mars']\n",
            "OOV Lookup ê²°ê³¼: ('<UNK>', 'from', 'Mars')\n",
            "<UNK> í† í°ì´ ì‚¬ìš©ë˜ì—ˆìŠµë‹ˆë‹¤: True\n"
          ]
        }
      ],
      "source": [
        "# ì‹¤ì œ ë¬¸ì¥ì—ì„œ ì–´íœ˜ì§‘ lookup\n",
        "test_sentence = my_corpus.sents(fileids=file_ids)[1104]\n",
        "lookup_result = lm.vocab.lookup(test_sentence)\n",
        "print(f\"ì›ë³¸ ë¬¸ì¥: {test_sentence}\")\n",
        "print(f\"Lookup ê²°ê³¼: {lookup_result}\")\n",
        "\n",
        "# OOV ë‹¨ì–´ë“¤ í…ŒìŠ¤íŠ¸\n",
        "oov_words = [\"aliens\", \"from\", \"Mars\"]\n",
        "oov_result = lm.vocab.lookup(oov_words)\n",
        "print(f\"\\nOOV í…ŒìŠ¤íŠ¸ ë‹¨ì–´ë“¤: {oov_words}\")\n",
        "print(f\"OOV Lookup ê²°ê³¼: {oov_result}\")\n",
        "print(f\"<UNK> í† í°ì´ ì‚¬ìš©ë˜ì—ˆìŠµë‹ˆë‹¤: {'<UNK>' in oov_result}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. N-gram ë¹ˆë„ìˆ˜ í™•ì¸\n",
        "\n",
        "ëª¨ë¸ì—ì„œ íŠ¹ì • N-gramì˜ ë¹ˆë„ìˆ˜ë¥¼ í™•ì¸í•´ë³´ê² ìŠµë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ì¹´ìš´íŠ¸ ê°ì²´ íƒ€ì…: <class 'nltk.lm.counter.NgramCounter'>\n",
            "'to be'ì˜ ë¹ˆë„ìˆ˜: 43\n",
            "'to'ì˜ ì „ì²´ ë¹ˆë„ìˆ˜: 754\n"
          ]
        }
      ],
      "source": [
        "# ì „ì²´ ì¹´ìš´íŠ¸ ì •ë³´\n",
        "print(f\"ì¹´ìš´íŠ¸ ê°ì²´ íƒ€ì…: {type(lm.counts)}\")\n",
        "\n",
        "# 'to be'ì˜ ë¹ˆë„ìˆ˜\n",
        "to_be_count = lm.counts[[\"to\"]][\"be\"]\n",
        "print(f\"'to be'ì˜ ë¹ˆë„ìˆ˜: {to_be_count}\")\n",
        "\n",
        "# 'to'ì˜ ì „ì²´ ë¹ˆë„ìˆ˜\n",
        "to_count = lm.counts['to']\n",
        "print(f\"'to'ì˜ ì „ì²´ ë¹ˆë„ìˆ˜: {to_count}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. í™•ë¥  ê³„ì‚°\n",
        "\n",
        "ë‹¨ì–´ì˜ í™•ë¥ ì„ ê³„ì‚°í•´ë³´ê² ìŠµë‹ˆë‹¤. ì¡°ê±´ë¶€ í™•ë¥ ë„ í™•ì¸í•´ë³´ê² ìŠµë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "P(be) = 0.003210\n",
            "P(be|to) = 0.057029\n",
            "P(be|not, to) = 0.272727\n"
          ]
        }
      ],
      "source": [
        "# ë‹¨ìˆœ í™•ë¥ \n",
        "be_prob = lm.score(\"be\")\n",
        "print(f\"P(be) = {be_prob:.6f}\")\n",
        "\n",
        "# ì¡°ê±´ë¶€ í™•ë¥ : P(be|to)\n",
        "be_given_to = lm.score(\"be\", [\"to\"])\n",
        "print(f\"P(be|to) = {be_given_to:.6f}\")\n",
        "\n",
        "# ì¡°ê±´ë¶€ í™•ë¥ : P(be|not, to)\n",
        "be_given_not_to = lm.score(\"be\", [\"not\", \"to\"])\n",
        "print(f\"P(be|not, to) = {be_given_not_to:.6f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 14. ë¡œê·¸ í™•ë¥  ê³„ì‚°\n",
        "\n",
        "ë§¤ìš° ì‘ì€ í™•ë¥ ê°’ì„ ë‹¤ë£¨ê¸° ìœ„í•´ ë¡œê·¸ ìŠ¤ì¼€ì¼ë¡œ í™•ë¥ ì„ ê³„ì‚°í•©ë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "log P(be) = -8.283356\n",
            "log P(be|to) = -4.132156\n",
            "log P(be|not, to) = -1.874469\n"
          ]
        }
      ],
      "source": [
        "# ë¡œê·¸ í™•ë¥ \n",
        "be_logprob = lm.logscore(\"be\")\n",
        "print(f\"log P(be) = {be_logprob:.6f}\")\n",
        "\n",
        "# ì¡°ê±´ë¶€ ë¡œê·¸ í™•ë¥ \n",
        "be_given_to_log = lm.logscore(\"be\", [\"to\"])\n",
        "print(f\"log P(be|to) = {be_given_to_log:.6f}\")\n",
        "\n",
        "be_given_not_to_log = lm.logscore(\"be\", [\"not\", \"to\"])\n",
        "print(f\"log P(be|not, to) = {be_given_not_to_log:.6f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 15. ì—”íŠ¸ë¡œí”¼ì™€ í¼í”Œë ‰ì„œí‹° ê³„ì‚°\n",
        "\n",
        "ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ê¸° ìœ„í•´ ì—”íŠ¸ë¡œí”¼ì™€ í¼í”Œë ‰ì„œí‹°ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "í…ŒìŠ¤íŠ¸ ë°ì´í„°: [('to', 'be'), ('or', 'not'), ('to', 'be')]\n",
            "ì—”íŠ¸ë¡œí”¼: 4.995137\n",
            "í¼í”Œë ‰ì„œí‹°: 31.892318\n",
            "\n",
            "ğŸ’¡ ì°¸ê³ : ë‚®ì€ í¼í”Œë ‰ì„œí‹°ëŠ” ë” ì¢‹ì€ ëª¨ë¸ì„ ì˜ë¯¸í•©ë‹ˆë‹¤!\n"
          ]
        }
      ],
      "source": [
        "# í…ŒìŠ¤íŠ¸ ë°ì´í„°\n",
        "test = [(\"to\", \"be\"), (\"or\", \"not\"), (\"to\", \"be\")]\n",
        "print(f\"í…ŒìŠ¤íŠ¸ ë°ì´í„°: {test}\")\n",
        "\n",
        "# ì—”íŠ¸ë¡œí”¼ ê³„ì‚°\n",
        "entropy = lm.entropy(test)\n",
        "print(f\"ì—”íŠ¸ë¡œí”¼: {entropy:.6f}\")\n",
        "\n",
        "# í¼í”Œë ‰ì„œí‹° ê³„ì‚°\n",
        "perplexity = lm.perplexity(test)\n",
        "print(f\"í¼í”Œë ‰ì„œí‹°: {perplexity:.6f}\")\n",
        "\n",
        "print(\"\\nğŸ’¡ ì°¸ê³ : ë‚®ì€ í¼í”Œë ‰ì„œí‹°ëŠ” ë” ì¢‹ì€ ëª¨ë¸ì„ ì˜ë¯¸í•©ë‹ˆë‹¤!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ê²°ë¡ \n",
        "\n",
        "ì´ ë…¸íŠ¸ë¶ì—ì„œ ìš°ë¦¬ëŠ”:\n",
        "1. NLTKë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ì½”í¼ìŠ¤ë¥¼ ì²˜ë¦¬í–ˆìŠµë‹ˆë‹¤\n",
        "2. íŒ¨ë”©ê³¼ N-gram ìƒì„±ì„ ë°°ì› ìŠµë‹ˆë‹¤\n",
        "3. Maximum Likelihood Estimatorë¡œ 3-gram ëª¨ë¸ì„ í›ˆë ¨í–ˆìŠµë‹ˆë‹¤\n",
        "4. í…ìŠ¤íŠ¸ ìƒì„±ì„ ìˆ˜í–‰í–ˆìŠµë‹ˆë‹¤\n",
        "5. í™•ë¥  ê³„ì‚°ê³¼ ëª¨ë¸ í‰ê°€ ë°©ë²•ì„ ë°°ì› ìŠµë‹ˆë‹¤\n",
        "\n",
        "N-gram ëª¨ë¸ì€ ê°„ë‹¨í•˜ì§€ë§Œ ì–¸ì–´ ëª¨ë¸ë§ì˜ ê¸°ì´ˆë¥¼ ì´í•´í•˜ëŠ” ë° ë§¤ìš° ìœ ìš©í•©ë‹ˆë‹¤!\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
