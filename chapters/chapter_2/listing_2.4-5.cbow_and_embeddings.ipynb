{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CBOW와 단어 임베딩 구현하기\n",
        "\n",
        "이 노트북에서는 CBOW(Continuous Bag of Words) 모델을 구현하여 단어 임베딩을 학습하는 방법을 배워보겠습니다.\n",
        "\n",
        "CBOW 모델은 Word2Vec의 한 종류로, 주변 단어들(컨텍스트)을 사용하여 중심 단어를 예측하는 방식으로 단어의 밀집 표현(dense representation)을 학습합니다.\n",
        "\n",
        "## 주요 학습 내용:\n",
        "- 텍스트 전처리와 어휘집 구성\n",
        "- 신경망 기반 언어 모델 구현\n",
        "- 순전파와 역전파 알고리즘\n",
        "- 경사 하강법을 통한 최적화\n",
        "- 학습된 임베딩의 시각화\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. 필요한 라이브러리 import\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "from utils import get_batches\n",
        "from utils import compute_pca\n",
        "from utils import get_dict\n",
        "import re\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# NLTK 다운로드 (필요한 경우)\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    print(\"NLTK punkt 토크나이저를 다운로드합니다...\")\n",
        "    nltk.download('punkt')\n",
        "\n",
        "print(\"필요한 라이브러리를 성공적으로 import했습니다.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. 데이터 로드 및 전처리\n",
        "\n",
        "햄릿 텍스트를 로드하고 CBOW 모델 훈련에 적합하도록 전처리합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 햄릿 텍스트 로드\n",
        "try:\n",
        "    with open(\"../../data/hamlet.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "        data = f.read()\n",
        "    print(\"햄릿 텍스트를 성공적으로 로드했습니다.\")\n",
        "    print(f\"원본 텍스트 길이: {len(data):,} 문자\")\n",
        "    print(f\"처음 200자: {data[:200]}...\")\n",
        "except FileNotFoundError:\n",
        "    print(\"햄릿 파일을 찾을 수 없습니다. 샘플 데이터로 대체합니다.\")\n",
        "    data = \"\"\"\n",
        "    To be or not to be, that is the question. Whether 'tis nobler in the mind to suffer \n",
        "    the slings and arrows of outrageous fortune, or to take arms against a sea of troubles \n",
        "    and by opposing end them. To die, to sleep, no more, and by a sleep to say we end \n",
        "    the heartache and the thousand natural shocks that flesh is heir to.\n",
        "    \"\"\" * 100  # 더 많은 데이터를 위해 반복\n",
        "\n",
        "print(f\"로드된 데이터 길이: {len(data):,} 문자\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 텍스트 전처리\n",
        "print(\"텍스트 전처리를 시작합니다...\")\n",
        "\n",
        "# 1단계: 구두점을 마침표로 통일\n",
        "print(\"1단계: 구두점 정리\")\n",
        "print(f\"전처리 전: {data[500:600]}\")\n",
        "data = re.sub(r\"[,!?;-]\", \".\", data)\n",
        "print(f\"전처리 후: {data[500:600]}\")\n",
        "\n",
        "# 2단계: 단어 단위로 토큰화\n",
        "print(\"\\\\n2단계: 토큰화\")\n",
        "data = nltk.word_tokenize(data)\n",
        "print(f\"토큰화 후 처음 20개 토큰: {data[:20]}\")\n",
        "\n",
        "# 3단계: 알파벳 단어만 유지하고 소문자로 변환\n",
        "print(\"\\\\n3단계: 알파벳 필터링 및 소문자 변환\")\n",
        "original_length = len(data)\n",
        "data = [ch.lower() for ch in data if ch.isalpha() or ch == \".\"]\n",
        "print(f\"필터링 전 토큰 수: {original_length:,}\")\n",
        "print(f\"필터링 후 토큰 수: {len(data):,}\")\n",
        "print(f\"샘플 토큰들 (500-515): {data[500:515]}\")\n",
        "\n",
        "print(f\"\\\\n✅ 전처리 완료! 총 {len(data):,}개의 토큰\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. 어휘집 구성 및 빈도 분석\n",
        "\n",
        "텍스트에서 어휘집을 추출하고 단어 빈도를 분석합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 단어 빈도 분석\n",
        "print(\"단어 빈도 분석을 수행합니다...\")\n",
        "fdist = nltk.FreqDist(word for word in data)\n",
        "\n",
        "print(f\"어휘집 크기: {len(fdist):,}개 고유 단어\")\n",
        "print(f\"전체 토큰 수: {sum(fdist.values()):,}개\")\n",
        "\n",
        "# 가장 빈번한 단어들 확인\n",
        "print(\"\\\\n📊 가장 빈번한 20개 단어:\")\n",
        "most_common = fdist.most_common(20)\n",
        "for i, (word, freq) in enumerate(most_common, 1):\n",
        "    print(f\"  {i:2d}. '{word}': {freq:,}회 ({freq/len(data)*100:.2f}%)\")\n",
        "\n",
        "# 빈도 분포 시각화 준비\n",
        "words, frequencies = zip(*most_common)\n",
        "print(f\"\\\\n상위 10개 단어가 전체의 {sum(frequencies[:10])/len(data)*100:.1f}%를 차지\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. 단어-인덱스 매핑 딕셔너리 생성\n",
        "\n",
        "효율적인 처리를 위해 단어와 인덱스 간의 양방향 매핑을 생성합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 단어-인덱스 매핑 딕셔너리 생성\n",
        "print(\"단어-인덱스 매핑 딕셔너리를 생성합니다...\")\n",
        "word2Ind, Ind2word = get_dict(data)\n",
        "V = len(word2Ind)\n",
        "\n",
        "print(f\"어휘집 크기 (V): {V:,}\")\n",
        "print(f\"word2Ind 딕셔너리 타입: {type(word2Ind)}\")\n",
        "print(f\"Ind2word 딕셔너리 타입: {type(Ind2word)}\")\n",
        "\n",
        "# 매핑 예시 확인\n",
        "test_words = ['king', 'queen', 'the', 'to', 'be']\n",
        "print(\"\\\\n단어 → 인덱스 매핑 예시:\")\n",
        "for word in test_words:\n",
        "    if word in word2Ind:\n",
        "        idx = word2Ind[word]\n",
        "        print(f\"  '{word}' → {idx}\")\n",
        "        # 역매핑도 확인\n",
        "        reverse_word = Ind2word[idx]\n",
        "        print(f\"  {idx} → '{reverse_word}' ✓\")\n",
        "    else:\n",
        "        print(f\"  '{word}' → 어휘집에 없음\")\n",
        "\n",
        "# 특정 인덱스들의 단어 확인\n",
        "print(\"\\\\n인덱스 → 단어 매핑 예시:\")\n",
        "sample_indices = [0, 1, 100, 500, 1000]\n",
        "for idx in sample_indices:\n",
        "    if idx < V:\n",
        "        word = Ind2word[idx]\n",
        "        print(f\"  인덱스 {idx} → '{word}'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. 신경망 모델 초기화\n",
        "\n",
        "CBOW 모델을 위한 신경망 가중치와 편향을 초기화합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def initialize_model(N, V, random_seed=1):\n",
        "    \"\"\"\n",
        "    CBOW 모델의 가중치와 편향을 초기화합니다.\n",
        "    \n",
        "    Args:\n",
        "        N: 은닉층(임베딩) 벡터의 차원\n",
        "        V: 어휘집 크기\n",
        "        random_seed: 재현 가능한 결과를 위한 시드\n",
        "    \n",
        "    Returns:\n",
        "        W1: 입력층 → 은닉층 가중치 (N × V)\n",
        "        W2: 은닉층 → 출력층 가중치 (V × N)  \n",
        "        b1: 은닉층 편향 (N × 1)\n",
        "        b2: 출력층 편향 (V × 1)\n",
        "    \"\"\"\n",
        "    np.random.seed(random_seed)\n",
        "    \n",
        "    # 가중치를 0과 1 사이의 난수로 초기화\n",
        "    W1 = np.random.rand(N, V)\n",
        "    W2 = np.random.rand(V, N)\n",
        "    b1 = np.random.rand(N, 1)\n",
        "    b2 = np.random.rand(V, 1)\n",
        "    \n",
        "    return W1, W2, b1, b2\n",
        "\n",
        "# 모델 파라미터 설정\n",
        "N = 50  # 임베딩 차원\n",
        "print(f\"모델 설정:\")\n",
        "print(f\"  임베딩 차원 (N): {N}\")\n",
        "print(f\"  어휘집 크기 (V): {V:,}\")\n",
        "\n",
        "# 모델 초기화 테스트\n",
        "W1, W2, b1, b2 = initialize_model(N, V, random_seed=42)\n",
        "\n",
        "print(f\"\\\\n초기화된 파라미터 크기:\")\n",
        "print(f\"  W1 (입력→은닉): {W1.shape}\")\n",
        "print(f\"  W2 (은닉→출력): {W2.shape}\")\n",
        "print(f\"  b1 (은닉층 편향): {b1.shape}\")\n",
        "print(f\"  b2 (출력층 편향): {b2.shape}\")\n",
        "\n",
        "# 전체 파라미터 수 계산\n",
        "total_params = W1.size + W2.size + b1.size + b2.size\n",
        "print(f\"\\\\n전체 파라미터 수: {total_params:,}개\")\n",
        "\n",
        "# 가중치 값 분포 확인\n",
        "print(f\"\\\\nW1 통계: min={W1.min():.4f}, max={W1.max():.4f}, mean={W1.mean():.4f}\")\n",
        "print(f\"W2 통계: min={W2.min():.4f}, max={W2.max():.4f}, mean={W2.mean():.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. 활성화 함수: Softmax 구현\n",
        "\n",
        "출력층에서 확률 분포를 생성하기 위한 Softmax 함수를 구현합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def softmax(z):\n",
        "    \"\"\"\n",
        "    Softmax 활성화 함수\n",
        "    모든 출력의 합이 1이 되도록 정규화하여 확률 분포를 생성\n",
        "    \n",
        "    Args:\n",
        "        z: 은닉층의 출력 점수 (V × batch_size)\n",
        "    \n",
        "    Returns:\n",
        "        yhat: 확률 분포 (각 단어에 대한 예측 확률)\n",
        "    \"\"\"\n",
        "    # 수치적 안정성을 위해 최대값을 빼줌 (exp 오버플로우 방지)\n",
        "    z_shifted = z - np.max(z, axis=0, keepdims=True)\n",
        "    exp_z = np.exp(z_shifted)\n",
        "    yhat = exp_z / np.sum(exp_z, axis=0, keepdims=True)\n",
        "    return yhat\n",
        "\n",
        "# Softmax 함수 테스트\n",
        "print(\"Softmax 함수 테스트:\")\n",
        "\n",
        "# 간단한 테스트 벡터\n",
        "test_z = np.array([[2.0], [1.0], [0.1]])\n",
        "test_probs = softmax(test_z)\n",
        "\n",
        "print(f\"입력 z: {test_z.flatten()}\")\n",
        "print(f\"출력 확률: {test_probs.flatten()}\")\n",
        "print(f\"확률 합: {np.sum(test_probs):.6f} (1에 가까워야 함)\")\n",
        "\n",
        "# 더 복잡한 테스트 (배치 처리)\n",
        "print(\"\\\\n배치 처리 테스트:\")\n",
        "batch_z = np.random.randn(5, 3)  # 5개 단어, 3개 샘플\n",
        "batch_probs = softmax(batch_z)\n",
        "\n",
        "print(f\"배치 입력 크기: {batch_z.shape}\")\n",
        "print(f\"배치 출력 크기: {batch_probs.shape}\")\n",
        "print(f\"각 샘플의 확률 합: {np.sum(batch_probs, axis=0)}\")\n",
        "\n",
        "# 극단적인 값에서의 안정성 테스트\n",
        "print(\"\\\\n수치적 안정성 테스트:\")\n",
        "extreme_z = np.array([[100.0], [200.0], [300.0]])\n",
        "extreme_probs = softmax(extreme_z)\n",
        "print(f\"극단적 입력: {extreme_z.flatten()}\")\n",
        "print(f\"안정적 출력: {extreme_probs.flatten()}\")\n",
        "print(f\"무한대나 NaN 없음: {np.all(np.isfinite(extreme_probs))}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. 순전파(Forward Propagation) 구현\n",
        "\n",
        "입력에서 출력까지 신경망을 통과하는 순전파 과정을 구현합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def forward_prop(x, W1, W2, b1, b2):\n",
        "    \"\"\"\n",
        "    CBOW 모델의 순전파\n",
        "    \n",
        "    Args:\n",
        "        x: 컨텍스트 단어들의 평균 원핫 벡터 (V × batch_size)\n",
        "        W1, W2, b1, b2: 모델 파라미터들\n",
        "    \n",
        "    Returns:\n",
        "        z: 출력층의 점수 벡터 (V × batch_size)\n",
        "        h: 은닉층 벡터 (활성화 후, N × batch_size)\n",
        "    \"\"\"\n",
        "    # 1단계: 입력층 → 은닉층\n",
        "    h_raw = W1 @ x + b1  # 선형 변환\n",
        "    h = np.maximum(0, h_raw)  # ReLU 활성화 함수\n",
        "    \n",
        "    # 2단계: 은닉층 → 출력층\n",
        "    z = W2 @ h + b2  # 선형 변환 (Softmax는 나중에 적용)\n",
        "    \n",
        "    return z, h\n",
        "\n",
        "# 순전파 테스트\n",
        "print(\"순전파 함수 테스트:\")\n",
        "\n",
        "# 가짜 입력 데이터 생성 (원핫 벡터)\n",
        "batch_size = 3\n",
        "x_test = np.zeros((V, batch_size))\n",
        "\n",
        "# 몇 개 단어에 대해 원핫 인코딩 (예시)\n",
        "test_word_indices = [10, 25, 50]\n",
        "for i, word_idx in enumerate(test_word_indices):\n",
        "    if word_idx < V:\n",
        "        x_test[word_idx, i] = 1.0\n",
        "\n",
        "print(f\"테스트 입력 크기: {x_test.shape}\")\n",
        "print(f\"입력의 원핫 검증: {np.sum(x_test, axis=0)} (각각 1이어야 함)\")\n",
        "\n",
        "# 순전파 실행\n",
        "z_test, h_test = forward_prop(x_test, W1, W2, b1, b2)\n",
        "\n",
        "print(f\"\\\\n순전파 결과:\")\n",
        "print(f\"은닉층 출력 (h) 크기: {h_test.shape}\")\n",
        "print(f\"출력층 점수 (z) 크기: {z_test.shape}\")\n",
        "\n",
        "# 은닉층 통계\n",
        "print(f\"\\\\n은닉층 (h) 통계:\")\n",
        "print(f\"  최소값: {h_test.min():.4f}\")\n",
        "print(f\"  최대값: {h_test.max():.4f}\")\n",
        "print(f\"  평균값: {h_test.mean():.4f}\")\n",
        "print(f\"  ReLU로 인한 0 개수: {np.sum(h_test == 0)}\")\n",
        "\n",
        "# 출력층 통계\n",
        "print(f\"\\\\n출력층 (z) 통계:\")\n",
        "print(f\"  최소값: {z_test.min():.4f}\")\n",
        "print(f\"  최대값: {z_test.max():.4f}\")\n",
        "print(f\"  평균값: {z_test.mean():.4f}\")\n",
        "\n",
        "# 확률로 변환해보기\n",
        "probs_test = softmax(z_test)\n",
        "print(f\"\\\\nSoftmax 후 확률:\")\n",
        "print(f\"  확률 합: {np.sum(probs_test, axis=0)}\")\n",
        "print(f\"  최고 확률들: {np.max(probs_test, axis=0)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. 비용 함수(Cost Function) 구현\n",
        "\n",
        "모델의 예측과 실제 값 사이의 차이를 측정하는 비용 함수를 구현합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_cost(y, yhat, batch_size):\n",
        "    \"\"\"\n",
        "    Cross-entropy 비용 함수\n",
        "    \n",
        "    Args:\n",
        "        y: 실제 레이블 (원핫 벡터, V × batch_size)\n",
        "        yhat: 예측 확률 (V × batch_size)\n",
        "        batch_size: 배치 크기\n",
        "    \n",
        "    Returns:\n",
        "        cost: 평균 cross-entropy 손실\n",
        "    \"\"\"\n",
        "    # 수치적 안정성을 위해 log(0) 방지\n",
        "    epsilon = 1e-15\n",
        "    yhat_clipped = np.clip(yhat, epsilon, 1 - epsilon)\n",
        "    \n",
        "    # Cross-entropy 계산\n",
        "    logprobs = np.multiply(np.log(yhat_clipped), y) + np.multiply(\n",
        "        np.log(1 - yhat_clipped), 1 - y\n",
        "    )\n",
        "    \n",
        "    # 평균 손실 계산\n",
        "    cost = -1 / batch_size * np.sum(logprobs)\n",
        "    cost = np.squeeze(cost)  # 스칼라로 변환\n",
        "    \n",
        "    return cost\n",
        "\n",
        "# 비용 함수 테스트\n",
        "print(\"비용 함수 테스트:\")\n",
        "\n",
        "# 완벽한 예측 케이스 (cost ≈ 0)\n",
        "print(\"\\\\n1. 완벽한 예측 케이스:\")\n",
        "y_perfect = np.array([[1.0, 0.0], [0.0, 1.0], [0.0, 0.0]])  # 3×2\n",
        "yhat_perfect = np.array([[0.99, 0.01], [0.01, 0.99], [0.0, 0.0]])  # 3×2\n",
        "cost_perfect = compute_cost(y_perfect, yhat_perfect, 2)\n",
        "print(f\"실제: {y_perfect.T}\")\n",
        "print(f\"예측: {yhat_perfect.T}\")\n",
        "print(f\"비용: {cost_perfect:.6f} (0에 가까워야 함)\")\n",
        "\n",
        "# 최악의 예측 케이스 (cost 높음)\n",
        "print(\"\\\\n2. 최악의 예측 케이스:\")\n",
        "y_worst = np.array([[1.0, 0.0], [0.0, 1.0], [0.0, 0.0]])  # 3×2\n",
        "yhat_worst = np.array([[0.01, 0.99], [0.99, 0.01], [0.0, 0.0]])  # 3×2\n",
        "cost_worst = compute_cost(y_worst, yhat_worst, 2)\n",
        "print(f\"실제: {y_worst.T}\")\n",
        "print(f\"예측: {yhat_worst.T}\")\n",
        "print(f\"비용: {cost_worst:.6f} (높아야 함)\")\n",
        "\n",
        "# 랜덤 예측 케이스\n",
        "print(\"\\\\n3. 랜덤 예측 케이스:\")\n",
        "np.random.seed(42)\n",
        "y_random = np.array([[1.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 0.0, 0.0]])  # 3×3\n",
        "yhat_random = np.random.rand(3, 3)\n",
        "yhat_random = yhat_random / np.sum(yhat_random, axis=0)  # 확률로 정규화\n",
        "cost_random = compute_cost(y_random, yhat_random, 3)\n",
        "print(f\"실제 첫 번째 샘플: {y_random[:, 0]}\")\n",
        "print(f\"예측 첫 번째 샘플: {yhat_random[:, 0]}\")\n",
        "print(f\"비용: {cost_random:.6f}\")\n",
        "\n",
        "# 실제 모델 출력으로 테스트\n",
        "print(\"\\\\n4. 실제 모델 출력으로 테스트:\")\n",
        "# 앞서 생성한 순전파 결과 사용\n",
        "y_actual = np.zeros((V, batch_size))\n",
        "target_indices = [100, 200, 300]  # 임의의 타겟 단어들\n",
        "for i, target_idx in enumerate(target_indices):\n",
        "    if target_idx < V:\n",
        "        y_actual[target_idx, i] = 1.0\n",
        "\n",
        "yhat_actual = softmax(z_test)\n",
        "cost_actual = compute_cost(y_actual, yhat_actual, batch_size)\n",
        "print(f\"실제 모델 비용: {cost_actual:.6f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. 역전파(Backpropagation) 구현\n",
        "\n",
        "비용을 최소화하기 위해 그래디언트를 계산하는 역전파를 구현합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def back_prop(x, yhat, y, h, W1, W2, b1, b2, batch_size):\n",
        "    \"\"\"\n",
        "    역전파를 통한 그래디언트 계산\n",
        "    \n",
        "    Args:\n",
        "        x: 입력 (평균 원핫 벡터, V × batch_size)\n",
        "        yhat: 예측 확률 (V × batch_size)\n",
        "        y: 실제 레이블 (V × batch_size)\n",
        "        h: 은닉층 출력 (N × batch_size)\n",
        "        W1, W2, b1, b2: 모델 파라미터들\n",
        "        batch_size: 배치 크기\n",
        "    \n",
        "    Returns:\n",
        "        grad_W1, grad_W2, grad_b1, grad_b2: 각 파라미터의 그래디언트\n",
        "    \"\"\"\n",
        "    # 출력층 오차 (V × batch_size)\n",
        "    output_error = yhat - y\n",
        "    \n",
        "    # 은닉층으로의 역전파 (ReLU 활성화 고려)\n",
        "    l1 = np.dot(W2.T, output_error)  # (N × batch_size)\n",
        "    l1 = np.maximum(0, l1)  # ReLU 미분 (h > 0인 곳만 통과)\n",
        "    \n",
        "    # 그래디언트 계산\n",
        "    grad_W1 = np.dot(l1, x.T) / batch_size  # (N × V)\n",
        "    grad_W2 = np.dot(output_error, h.T) / batch_size  # (V × N)\n",
        "    grad_b1 = np.sum(l1, axis=1, keepdims=True) / batch_size  # (N × 1)\n",
        "    grad_b2 = np.sum(output_error, axis=1, keepdims=True) / batch_size  # (V × 1)\n",
        "    \n",
        "    return grad_W1, grad_W2, grad_b1, grad_b2\n",
        "\n",
        "# 역전파 테스트\n",
        "print(\"역전파 함수 테스트:\")\n",
        "\n",
        "# 앞서 생성한 순전파 결과 활용\n",
        "yhat_test = softmax(z_test)\n",
        "\n",
        "# 가짜 타겟 생성\n",
        "y_test = np.zeros((V, batch_size))\n",
        "target_words = [word2Ind.get('king', 0), word2Ind.get('queen', 1), word2Ind.get('the', 2)]\n",
        "for i, target_idx in enumerate(target_words):\n",
        "    y_test[target_idx, i] = 1.0\n",
        "\n",
        "print(f\"테스트 설정:\")\n",
        "print(f\"  입력 크기: {x_test.shape}\")\n",
        "print(f\"  예측 크기: {yhat_test.shape}\")\n",
        "print(f\"  타겟 크기: {y_test.shape}\")\n",
        "print(f\"  은닉층 크기: {h_test.shape}\")\n",
        "\n",
        "# 역전파 실행\n",
        "grad_W1, grad_W2, grad_b1, grad_b2 = back_prop(\n",
        "    x_test, yhat_test, y_test, h_test, W1, W2, b1, b2, batch_size\n",
        ")\n",
        "\n",
        "print(f\"\\\\n계산된 그래디언트 크기:\")\n",
        "print(f\"  grad_W1: {grad_W1.shape}\")\n",
        "print(f\"  grad_W2: {grad_W2.shape}\")\n",
        "print(f\"  grad_b1: {grad_b1.shape}\")\n",
        "print(f\"  grad_b2: {grad_b2.shape}\")\n",
        "\n",
        "print(f\"\\\\n그래디언트 통계:\")\n",
        "print(f\"  grad_W1: min={grad_W1.min():.6f}, max={grad_W1.max():.6f}, mean={grad_W1.mean():.6f}\")\n",
        "print(f\"  grad_W2: min={grad_W2.min():.6f}, max={grad_W2.max():.6f}, mean={grad_W2.mean():.6f}\")\n",
        "print(f\"  grad_b1: min={grad_b1.min():.6f}, max={grad_b1.max():.6f}, mean={grad_b1.mean():.6f}\")\n",
        "print(f\"  grad_b2: min={grad_b2.min():.6f}, max={grad_b2.max():.6f}, mean={grad_b2.mean():.6f}\")\n",
        "\n",
        "# 그래디언트가 0이 아닌지 확인 (학습이 일어나는지 확인)\n",
        "print(f\"\\\\n그래디언트 0이 아닌 원소 비율:\")\n",
        "print(f\"  grad_W1: {np.mean(grad_W1 != 0)*100:.1f}%\")\n",
        "print(f\"  grad_W2: {np.mean(grad_W2 != 0)*100:.1f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. 경사 하강법 구현\n",
        "\n",
        "모든 구성 요소를 결합하여 전체 훈련 과정을 구현합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def gradient_descent(data, word2Ind, N, V, num_iters, alpha=0.03):\n",
        "    \"\"\"\n",
        "    경사 하강법을 사용한 CBOW 모델 훈련\n",
        "    \n",
        "    Args:\n",
        "        data: 전처리된 텍스트 데이터\n",
        "        word2Ind: 단어→인덱스 딕셔너리\n",
        "        N: 임베딩 차원\n",
        "        V: 어휘집 크기\n",
        "        num_iters: 훈련 반복 횟수\n",
        "        alpha: 학습률\n",
        "    \n",
        "    Returns:\n",
        "        W1, W2, b1, b2: 훈련된 모델 파라미터\n",
        "    \"\"\"\n",
        "    # 모델 파라미터 초기화\n",
        "    W1, W2, b1, b2 = initialize_model(N, V, random_seed=8855)\n",
        "    \n",
        "    # 훈련 설정\n",
        "    batch_size = 128\n",
        "    C = 2  # 컨텍스트 윈도우 크기\n",
        "    iters = 0\n",
        "    costs = []  # 비용 기록용\n",
        "    \n",
        "    print(f\"훈련 시작:\")\n",
        "    print(f\"  학습률: {alpha}\")\n",
        "    print(f\"  배치 크기: {batch_size}\")\n",
        "    print(f\"  컨텍스트 윈도우: {C}\")\n",
        "    print(f\"  총 반복 횟수: {num_iters}\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    # 훈련 루프\n",
        "    for x, y in get_batches(data, word2Ind, V, C, batch_size):\n",
        "        # 순전파\n",
        "        z, h = forward_prop(x, W1, W2, b1, b2)\n",
        "        yhat = softmax(z)\n",
        "        \n",
        "        # 비용 계산\n",
        "        cost = compute_cost(y, yhat, batch_size)\n",
        "        costs.append(cost)\n",
        "        \n",
        "        # 주기적으로 비용 출력\n",
        "        if (iters + 1) % 10 == 0:\n",
        "            print(f\"반복 {iters+1:3d}: 비용 = {cost:.6f}\")\n",
        "        \n",
        "        # 역전파\n",
        "        grad_W1, grad_W2, grad_b1, grad_b2 = back_prop(\n",
        "            x, yhat, y, h, W1, W2, b1, b2, batch_size\n",
        "        )\n",
        "        \n",
        "        # 파라미터 업데이트\n",
        "        W1 = W1 - alpha * grad_W1\n",
        "        W2 = W2 - alpha * grad_W2\n",
        "        b1 = b1 - alpha * grad_b1\n",
        "        b2 = b2 - alpha * grad_b2\n",
        "        \n",
        "        iters += 1\n",
        "        \n",
        "        # 종료 조건\n",
        "        if iters == num_iters:\n",
        "            break\n",
        "            \n",
        "        # 학습률 스케줄링 (매 100회마다 감소)\n",
        "        if iters % 100 == 0:\n",
        "            alpha *= 0.66\n",
        "            print(f\"    → 학습률을 {alpha:.4f}로 조정\")\n",
        "    \n",
        "    print(\"-\" * 50)\n",
        "    print(f\"훈련 완료! 총 {iters}회 반복\")\n",
        "    print(f\"최종 비용: {costs[-1]:.6f}\")\n",
        "    print(f\"초기 비용: {costs[0]:.6f}\")\n",
        "    print(f\"비용 개선: {costs[0] - costs[-1]:.6f}\")\n",
        "    \n",
        "    return W1, W2, b1, b2, costs\n",
        "\n",
        "print(\"경사 하강법 함수가 정의되었습니다.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. 모델 훈련 실행\n",
        "\n",
        "실제로 CBOW 모델을 훈련시켜보겠습니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 훈련 파라미터 설정\n",
        "C = 2  # 컨텍스트 윈도우 크기\n",
        "N = 50  # 임베딩 차원\n",
        "num_iters = 150  # 반복 횟수 (빠른 실행을 위해 원본보다 적게)\n",
        "\n",
        "print(\"🚀 CBOW 모델 훈련을 시작합니다!\")\n",
        "print(f\"데이터 크기: {len(data):,} 토큰\")\n",
        "print(f\"어휘집 크기: {V:,} 단어\")\n",
        "print(f\"임베딩 차원: {N}\")\n",
        "print(f\"컨텍스트 윈도우: ±{C}\")\n",
        "\n",
        "# 단어-인덱스 딕셔너리 재생성 (정확성 확보)\n",
        "word2Ind, Ind2word = get_dict(data)\n",
        "V = len(word2Ind)\n",
        "\n",
        "print(f\"\\\\n훈련 시작...\")\n",
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "# 모델 훈련\n",
        "W1_trained, W2_trained, b1_trained, b2_trained, training_costs = gradient_descent(\n",
        "    data, word2Ind, N, V, num_iters\n",
        ")\n",
        "\n",
        "end_time = time.time()\n",
        "training_time = end_time - start_time\n",
        "\n",
        "print(f\"\\\\n✅ 훈련 완료!\")\n",
        "print(f\"훈련 시간: {training_time:.1f}초\")\n",
        "print(f\"최종 파라미터 크기:\")\n",
        "print(f\"  W1: {W1_trained.shape}\")\n",
        "print(f\"  W2: {W2_trained.shape}\")\n",
        "print(f\"  b1: {b1_trained.shape}\")\n",
        "print(f\"  b2: {b2_trained.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. 훈련 과정 시각화\n",
        "\n",
        "훈련 중 비용의 변화를 시각화하여 학습 과정을 분석합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 훈련 과정 시각화\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# 전체 비용 변화\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(training_costs, 'b-', linewidth=2, alpha=0.7)\n",
        "plt.title('전체 훈련 과정의 비용 변화', fontsize=14, pad=20)\n",
        "plt.xlabel('반복 횟수')\n",
        "plt.ylabel('Cross-entropy 손실')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# 최근 50회의 비용 변화 (세부 관찰)\n",
        "plt.subplot(1, 2, 2)\n",
        "if len(training_costs) > 50:\n",
        "    plt.plot(training_costs[-50:], 'r-', linewidth=2, alpha=0.7)\n",
        "    plt.title('최근 50회 반복의 비용 변화', fontsize=14, pad=20)\n",
        "    plt.xlabel('반복 횟수 (최근 50회)')\n",
        "else:\n",
        "    plt.plot(training_costs, 'r-', linewidth=2, alpha=0.7)\n",
        "    plt.title('전체 비용 변화', fontsize=14, pad=20)\n",
        "    plt.xlabel('반복 횟수')\n",
        "plt.ylabel('Cross-entropy 손실')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 훈련 통계 출력\n",
        "print(f\"📊 훈련 통계:\")\n",
        "print(f\"  초기 비용: {training_costs[0]:.6f}\")\n",
        "print(f\"  최종 비용: {training_costs[-1]:.6f}\")\n",
        "print(f\"  비용 감소: {training_costs[0] - training_costs[-1]:.6f}\")\n",
        "print(f\"  감소율: {((training_costs[0] - training_costs[-1]) / training_costs[0] * 100):.2f}%\")\n",
        "\n",
        "# 비용 변화 추세 분석\n",
        "if len(training_costs) > 10:\n",
        "    recent_trend = np.mean(training_costs[-10:]) - np.mean(training_costs[-20:-10])\n",
        "    if recent_trend < 0:\n",
        "        print(f\"  최근 추세: 👍 계속 감소 중 ({recent_trend:.6f})\")\n",
        "    else:\n",
        "        print(f\"  최근 추세: 📈 증가 또는 안정화 ({recent_trend:.6f})\")\n",
        "\n",
        "print(f\"\\\\n학습이 성공적으로 완료되었습니다! 이제 임베딩을 추출할 준비가 되었습니다.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. 단어 임베딩 추출\n",
        "\n",
        "훈련된 모델에서 단어 임베딩을 추출합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 단어 임베딩 추출\n",
        "# W1과 W2의 평균을 사용 (일반적인 Word2Vec 접근법)\n",
        "embeddings = (W1_trained.T + W2_trained) / 2.0\n",
        "\n",
        "print(f\"추출된 임베딩:\")\n",
        "print(f\"  크기: {embeddings.shape}\")\n",
        "print(f\"  각 단어는 {embeddings.shape[1]}차원 벡터로 표현됩니다\")\n",
        "\n",
        "# 특정 단어들의 임베딩 확인\n",
        "test_words = [\"king\", \"queen\", \"lord\", \"man\", \"woman\", \"prince\", \"ophelia\", \"rich\", \"happy\"]\n",
        "print(f\"\\\\n분석할 단어들: {test_words}\")\n",
        "\n",
        "# 단어들이 어휘집에 있는지 확인\n",
        "available_words = []\n",
        "word_indices = []\n",
        "\n",
        "for word in test_words:\n",
        "    if word in word2Ind:\n",
        "        available_words.append(word)\n",
        "        word_indices.append(word2Ind[word])\n",
        "        print(f\"  ✓ '{word}' → 인덱스 {word2Ind[word]}\")\n",
        "    else:\n",
        "        print(f\"  ✗ '{word}' → 어휘집에 없음\")\n",
        "\n",
        "print(f\"\\\\n분석 가능한 단어 수: {len(available_words)}/{len(test_words)}\")\n",
        "\n",
        "if len(available_words) > 0:\n",
        "    # 선택된 단어들의 임베딩 추출\n",
        "    X = embeddings[word_indices, :]\n",
        "    print(f\"\\\\n선택된 단어들의 임베딩 행렬 크기: {X.shape}\")\n",
        "    \n",
        "    # 임베딩 벡터 통계\n",
        "    print(f\"\\\\n임베딩 통계:\")\n",
        "    print(f\"  평균: {X.mean():.4f}\")\n",
        "    print(f\"  표준편차: {X.std():.4f}\")\n",
        "    print(f\"  최솟값: {X.min():.4f}\")\n",
        "    print(f\"  최댓값: {X.max():.4f}\")\n",
        "    \n",
        "    # 각 단어의 임베딩 벡터 일부 출력\n",
        "    print(f\"\\\\n각 단어의 임베딩 벡터 (처음 5차원):\")\n",
        "    for i, word in enumerate(available_words):\n",
        "        embedding_sample = X[i, :5]\n",
        "        print(f\"  '{word}': [{', '.join([f'{x:.3f}' for x in embedding_sample])}, ...]\")\n",
        "else:\n",
        "    print(\"⚠️  분석할 수 있는 단어가 없습니다. 다른 단어들을 시도해보세요.\")\n",
        "    \n",
        "    # 어휘집에서 임의의 단어들 선택\n",
        "    available_words = list(word2Ind.keys())[:9]  # 처음 9개 단어\n",
        "    word_indices = [word2Ind[word] for word in available_words]\n",
        "    X = embeddings[word_indices, :]\n",
        "    print(f\"\\\\n대신 다음 단어들로 분석합니다: {available_words}\")\n",
        "    print(f\"임베딩 행렬 크기: {X.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 14. PCA를 통한 임베딩 시각화\n",
        "\n",
        "고차원 임베딩을 2차원으로 축소하여 시각화합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# PCA를 사용하여 2차원으로 축소\n",
        "print(\"PCA를 사용하여 임베딩을 2차원으로 축소합니다...\")\n",
        "\n",
        "try:\n",
        "    # PCA 적용\n",
        "    result = compute_pca(X, 2)\n",
        "    print(f\"PCA 결과 크기: {result.shape}\")\n",
        "    print(f\"축소된 좌표 범위:\")\n",
        "    print(f\"  X축: {result[:, 0].min():.3f} ~ {result[:, 0].max():.3f}\")\n",
        "    print(f\"  Y축: {result[:, 1].min():.3f} ~ {result[:, 1].max():.3f}\")\n",
        "    \n",
        "    # 시각화\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    plt.scatter(result[:, 0], result[:, 1], c='red', s=100, alpha=0.7, edgecolors='black')\n",
        "    \n",
        "    # 각 점에 단어 레이블 추가\n",
        "    for i, word in enumerate(available_words):\n",
        "        plt.annotate(word, \n",
        "                    xy=(result[i, 0], result[i, 1]), \n",
        "                    xytext=(5, 5), \n",
        "                    textcoords='offset points',\n",
        "                    fontsize=12, \n",
        "                    fontweight='bold',\n",
        "                    bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.7))\n",
        "    \n",
        "    plt.title('CBOW 모델로 학습한 단어 임베딩 (PCA 시각화)', fontsize=16, pad=20)\n",
        "    plt.xlabel('주성분 1', fontsize=14)\n",
        "    plt.ylabel('주성분 2', fontsize=14)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 축 범위를 조정하여 레이블이 잘리지 않도록\n",
        "    x_margin = (result[:, 0].max() - result[:, 0].min()) * 0.1\n",
        "    y_margin = (result[:, 1].max() - result[:, 1].min()) * 0.1\n",
        "    plt.xlim(result[:, 0].min() - x_margin, result[:, 0].max() + x_margin)\n",
        "    plt.ylim(result[:, 1].min() - y_margin, result[:, 1].max() + y_margin)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # 단어 간 거리 분석\n",
        "    print(\"\\\\n📏 단어 간 유클리드 거리 (원본 임베딩 공간):\")\n",
        "    from scipy.spatial.distance import pdist, squareform\n",
        "    \n",
        "    # 원본 고차원 임베딩에서의 거리\n",
        "    distances = pdist(X, metric='euclidean')\n",
        "    distance_matrix = squareform(distances)\n",
        "    \n",
        "    # 가장 유사한 단어 쌍들 (거리가 가까운)\n",
        "    upper_triangle = np.triu(distance_matrix, k=1)\n",
        "    min_dist_idx = np.unravel_index(np.argmax(upper_triangle == 0), upper_triangle.shape)\n",
        "    \n",
        "    print(\"가장 가까운 단어 쌍들:\")\n",
        "    sorted_indices = np.argsort(distances)\n",
        "    for i in range(min(3, len(sorted_indices))):\n",
        "        idx = sorted_indices[i]\n",
        "        # 거리 행렬에서 인덱스 계산\n",
        "        n = len(available_words)\n",
        "        row = int((-1 + np.sqrt(1 + 8*idx)) / 2)\n",
        "        col = int(idx - row * (row + 1) / 2 + row + 1)\n",
        "        if row < len(available_words) and col < len(available_words):\n",
        "            distance = distances[idx]\n",
        "            print(f\"  {available_words[row]} ↔ {available_words[col]}: {distance:.3f}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"❌ PCA 시각화 중 오류 발생: {e}\")\n",
        "    print(\"대신 단순한 통계 분석을 제공합니다.\")\n",
        "    \n",
        "    # 단어별 임베딩 크기(norm) 비교\n",
        "    norms = np.linalg.norm(X, axis=1)\n",
        "    print(\"\\\\n단어별 임베딩 벡터 크기(L2 norm):\")\n",
        "    for i, word in enumerate(available_words):\n",
        "        print(f\"  '{word}': {norms[i]:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 15. 임베딩 품질 분석\n",
        "\n",
        "학습된 임베딩의 품질을 다양한 방법으로 분석해봅시다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 임베딩 품질 분석\n",
        "\n",
        "print(\"🔍 임베딩 품질 분석:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# 1. 코사인 유사도 분석\n",
        "def cosine_similarity(a, b):\n",
        "    \"\"\"두 벡터 간의 코사인 유사도 계산\"\"\"\n",
        "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
        "\n",
        "if len(available_words) >= 2:\n",
        "    print(\"\\\\n1. 단어 간 코사인 유사도:\")\n",
        "    for i in range(min(3, len(available_words))):\n",
        "        for j in range(i+1, min(3, len(available_words))):\n",
        "            similarity = cosine_similarity(X[i], X[j])\n",
        "            print(f\"   '{available_words[i]}' ↔ '{available_words[j]}': {similarity:.4f}\")\n",
        "\n",
        "# 2. 가장 유사한/다른 단어 찾기\n",
        "def find_most_similar_words(target_word, embeddings, word_list, word2ind, top_k=5):\n",
        "    \"\"\"특정 단어와 가장 유사한 단어들 찾기\"\"\"\n",
        "    if target_word not in word2ind:\n",
        "        return []\n",
        "    \n",
        "    target_idx = word2ind[target_word]\n",
        "    target_embedding = embeddings[target_idx]\n",
        "    \n",
        "    similarities = []\n",
        "    for word in word_list:\n",
        "        if word != target_word:\n",
        "            word_idx = word2ind[word]\n",
        "            word_embedding = embeddings[word_idx]\n",
        "            similarity = cosine_similarity(target_embedding, word_embedding)\n",
        "            similarities.append((word, similarity))\n",
        "    \n",
        "    # 유사도 순으로 정렬\n",
        "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
        "    return similarities[:top_k]\n",
        "\n",
        "print(\"\\\\n2. 단어 유사도 순위:\")\n",
        "for target_word in available_words[:3]:  # 처음 3개 단어에 대해\n",
        "    similar_words = find_most_similar_words(target_word, embeddings, available_words, word2Ind)\n",
        "    print(f\"   '{target_word}'와 유사한 단어들:\")\n",
        "    for word, sim in similar_words:\n",
        "        print(f\"     {word}: {sim:.4f}\")\n",
        "\n",
        "# 3. 임베딩 공간의 분산 분석\n",
        "print(\"\\\\n3. 임베딩 공간 분석:\")\n",
        "print(f\"   전체 어휘집 크기: {embeddings.shape[0]:,} 단어\")\n",
        "print(f\"   임베딩 차원: {embeddings.shape[1]}\")\n",
        "\n",
        "# 각 차원의 분산\n",
        "dimension_variances = np.var(embeddings, axis=0)\n",
        "print(f\"   차원별 분산 - 평균: {dimension_variances.mean():.4f}, 표준편차: {dimension_variances.std():.4f}\")\n",
        "print(f\"   가장 분산이 큰 차원: {np.argmax(dimension_variances)} (분산: {dimension_variances.max():.4f})\")\n",
        "print(f\"   가장 분산이 작은 차원: {np.argmin(dimension_variances)} (분산: {dimension_variances.min():.4f})\")\n",
        "\n",
        "# 4. 임베딩 벡터들의 길이 분포\n",
        "embedding_norms = np.linalg.norm(embeddings, axis=1)\n",
        "print(f\"\\\\n4. 임베딩 벡터 크기 분포:\")\n",
        "print(f\"   평균 크기: {embedding_norms.mean():.4f}\")\n",
        "print(f\"   표준편차: {embedding_norms.std():.4f}\")\n",
        "print(f\"   최솟값: {embedding_norms.min():.4f}\")\n",
        "print(f\"   최댓값: {embedding_norms.max():.4f}\")\n",
        "\n",
        "# 5. 학습의 효과 확인\n",
        "print(\"\\\\n5. 학습 효과 검증:\")\n",
        "# 초기 랜덤 임베딩과 비교\n",
        "W1_random, W2_random, _, _ = initialize_model(N, V, random_seed=42)\n",
        "random_embeddings = (W1_random.T + W2_random) / 2.0\n",
        "random_norms = np.linalg.norm(random_embeddings, axis=1)\n",
        "\n",
        "print(f\"   학습된 임베딩 평균 크기: {embedding_norms.mean():.4f}\")\n",
        "print(f\"   랜덤 임베딩 평균 크기: {random_norms.mean():.4f}\")\n",
        "print(f\"   차이: {abs(embedding_norms.mean() - random_norms.mean()):.4f}\")\n",
        "\n",
        "# 임베딩이 의미적으로 구조화되었는지 확인\n",
        "print(\"\\\\n💡 임베딩 학습이 완료되었습니다!\")\n",
        "print(\"   - 유사한 단어들이 가까운 위치에 배치되었는지 확인해보세요\")\n",
        "print(\"   - 단어 간 코사인 유사도를 통해 의미적 관계를 파악할 수 있습니다\")\n",
        "print(\"   - PCA 시각화를 통해 고차원 임베딩의 2차원 투영을 관찰할 수 있습니다\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 결론\n",
        "\n",
        "이 노트북에서 우리는:\n",
        "\n",
        "1. **CBOW 모델의 전체 구조**를 이해하고 구현했습니다\n",
        "2. **신경망의 핵심 구성 요소들**을 단계별로 구현했습니다:\n",
        "   - 가중치 초기화\n",
        "   - 순전파 (Forward Propagation)\n",
        "   - Softmax 활성화 함수\n",
        "   - Cross-entropy 비용 함수\n",
        "   - 역전파 (Backpropagation)\n",
        "   - 경사 하강법 최적화\n",
        "\n",
        "3. **실제 텍스트 데이터로 모델을 훈련**하여 의미있는 단어 임베딩을 학습했습니다\n",
        "\n",
        "4. **학습된 임베딩을 분석하고 시각화**하여 품질을 평가했습니다\n",
        "\n",
        "### 주요 학습 포인트:\n",
        "\n",
        "- **CBOW의 핵심 아이디어**: 주변 단어들로 중심 단어를 예측\n",
        "- **신경망 기반 언어 모델**: 확률적 언어 모델을 신경망으로 구현\n",
        "- **임베딩 학습**: 희소한 원핫 벡터를 밀집한 의미 벡터로 변환  \n",
        "- **최적화 과정**: 그래디언트 기반 학습의 실제 적용\n",
        "- **고차원 데이터 시각화**: PCA를 통한 차원 축소와 해석\n",
        "\n",
        "### CBOW vs 다른 방법들:\n",
        "\n",
        "- **N-gram 모델**: 이산적 확률 vs 연속적 벡터 표현\n",
        "- **Naive Bayes**: 단순 분류 vs 복합적 표현 학습\n",
        "- **마르코프 체인**: 순차적 생성 vs 컨텍스트 기반 예측\n",
        "\n",
        "CBOW 모델은 현대 NLP의 기초가 되는 **Word2Vec**의 핵심 아이디어를 보여주며, 트랜스포머와 같은 더 복잡한 모델들의 이해를 위한 중요한 디딤돌 역할을 합니다!\n",
        "\n",
        "### 다음 단계:\n",
        "- Skip-gram 모델과의 비교\n",
        "- 더 큰 데이터셋으로의 확장\n",
        "- 사전 훈련된 임베딩 (GloVe, FastText) 활용\n",
        "- 트랜스포머 기반 모델로의 발전\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
