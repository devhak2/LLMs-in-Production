{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CBOWì™€ ë‹¨ì–´ ì„ë² ë”© êµ¬í˜„í•˜ê¸°\n",
        "\n",
        "ì´ ë…¸íŠ¸ë¶ì—ì„œëŠ” CBOW(Continuous Bag of Words) ëª¨ë¸ì„ êµ¬í˜„í•˜ì—¬ ë‹¨ì–´ ì„ë² ë”©ì„ í•™ìŠµí•˜ëŠ” ë°©ë²•ì„ ë°°ì›Œë³´ê² ìŠµë‹ˆë‹¤.\n",
        "\n",
        "CBOW ëª¨ë¸ì€ Word2Vecì˜ í•œ ì¢…ë¥˜ë¡œ, ì£¼ë³€ ë‹¨ì–´ë“¤(ì»¨í…ìŠ¤íŠ¸)ì„ ì‚¬ìš©í•˜ì—¬ ì¤‘ì‹¬ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ë‹¨ì–´ì˜ ë°€ì§‘ í‘œí˜„(dense representation)ì„ í•™ìŠµí•©ë‹ˆë‹¤.\n",
        "\n",
        "## ì£¼ìš” í•™ìŠµ ë‚´ìš©:\n",
        "- í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ì™€ ì–´íœ˜ì§‘ êµ¬ì„±\n",
        "- ì‹ ê²½ë§ ê¸°ë°˜ ì–¸ì–´ ëª¨ë¸ êµ¬í˜„\n",
        "- ìˆœì „íŒŒì™€ ì—­ì „íŒŒ ì•Œê³ ë¦¬ì¦˜\n",
        "- ê²½ì‚¬ í•˜ê°•ë²•ì„ í†µí•œ ìµœì í™”\n",
        "- í•™ìŠµëœ ì„ë² ë”©ì˜ ì‹œê°í™”\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ import\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "from utils import get_batches\n",
        "from utils import compute_pca\n",
        "from utils import get_dict\n",
        "import re\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# NLTK ë‹¤ìš´ë¡œë“œ (í•„ìš”í•œ ê²½ìš°)\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    print(\"NLTK punkt í† í¬ë‚˜ì´ì €ë¥¼ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤...\")\n",
        "    nltk.download('punkt')\n",
        "\n",
        "print(\"í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„±ê³µì ìœ¼ë¡œ importí–ˆìŠµë‹ˆë‹¤.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬\n",
        "\n",
        "í–„ë¦¿ í…ìŠ¤íŠ¸ë¥¼ ë¡œë“œí•˜ê³  CBOW ëª¨ë¸ í›ˆë ¨ì— ì í•©í•˜ë„ë¡ ì „ì²˜ë¦¬í•©ë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# í–„ë¦¿ í…ìŠ¤íŠ¸ ë¡œë“œ\n",
        "try:\n",
        "    with open(\"../../data/hamlet.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "        data = f.read()\n",
        "    print(\"í–„ë¦¿ í…ìŠ¤íŠ¸ë¥¼ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.\")\n",
        "    print(f\"ì›ë³¸ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(data):,} ë¬¸ì\")\n",
        "    print(f\"ì²˜ìŒ 200ì: {data[:200]}...\")\n",
        "except FileNotFoundError:\n",
        "    print(\"í–„ë¦¿ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìƒ˜í”Œ ë°ì´í„°ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.\")\n",
        "    data = \"\"\"\n",
        "    To be or not to be, that is the question. Whether 'tis nobler in the mind to suffer \n",
        "    the slings and arrows of outrageous fortune, or to take arms against a sea of troubles \n",
        "    and by opposing end them. To die, to sleep, no more, and by a sleep to say we end \n",
        "    the heartache and the thousand natural shocks that flesh is heir to.\n",
        "    \"\"\" * 100  # ë” ë§ì€ ë°ì´í„°ë¥¼ ìœ„í•´ ë°˜ë³µ\n",
        "\n",
        "print(f\"ë¡œë“œëœ ë°ì´í„° ê¸¸ì´: {len(data):,} ë¬¸ì\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬\n",
        "print(\"í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...\")\n",
        "\n",
        "# 1ë‹¨ê³„: êµ¬ë‘ì ì„ ë§ˆì¹¨í‘œë¡œ í†µì¼\n",
        "print(\"1ë‹¨ê³„: êµ¬ë‘ì  ì •ë¦¬\")\n",
        "print(f\"ì „ì²˜ë¦¬ ì „: {data[500:600]}\")\n",
        "data = re.sub(r\"[,!?;-]\", \".\", data)\n",
        "print(f\"ì „ì²˜ë¦¬ í›„: {data[500:600]}\")\n",
        "\n",
        "# 2ë‹¨ê³„: ë‹¨ì–´ ë‹¨ìœ„ë¡œ í† í°í™”\n",
        "print(\"\\\\n2ë‹¨ê³„: í† í°í™”\")\n",
        "data = nltk.word_tokenize(data)\n",
        "print(f\"í† í°í™” í›„ ì²˜ìŒ 20ê°œ í† í°: {data[:20]}\")\n",
        "\n",
        "# 3ë‹¨ê³„: ì•ŒíŒŒë²³ ë‹¨ì–´ë§Œ ìœ ì§€í•˜ê³  ì†Œë¬¸ìë¡œ ë³€í™˜\n",
        "print(\"\\\\n3ë‹¨ê³„: ì•ŒíŒŒë²³ í•„í„°ë§ ë° ì†Œë¬¸ì ë³€í™˜\")\n",
        "original_length = len(data)\n",
        "data = [ch.lower() for ch in data if ch.isalpha() or ch == \".\"]\n",
        "print(f\"í•„í„°ë§ ì „ í† í° ìˆ˜: {original_length:,}\")\n",
        "print(f\"í•„í„°ë§ í›„ í† í° ìˆ˜: {len(data):,}\")\n",
        "print(f\"ìƒ˜í”Œ í† í°ë“¤ (500-515): {data[500:515]}\")\n",
        "\n",
        "print(f\"\\\\nâœ… ì „ì²˜ë¦¬ ì™„ë£Œ! ì´ {len(data):,}ê°œì˜ í† í°\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. ì–´íœ˜ì§‘ êµ¬ì„± ë° ë¹ˆë„ ë¶„ì„\n",
        "\n",
        "í…ìŠ¤íŠ¸ì—ì„œ ì–´íœ˜ì§‘ì„ ì¶”ì¶œí•˜ê³  ë‹¨ì–´ ë¹ˆë„ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ë‹¨ì–´ ë¹ˆë„ ë¶„ì„\n",
        "print(\"ë‹¨ì–´ ë¹ˆë„ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤...\")\n",
        "fdist = nltk.FreqDist(word for word in data)\n",
        "\n",
        "print(f\"ì–´íœ˜ì§‘ í¬ê¸°: {len(fdist):,}ê°œ ê³ ìœ  ë‹¨ì–´\")\n",
        "print(f\"ì „ì²´ í† í° ìˆ˜: {sum(fdist.values()):,}ê°œ\")\n",
        "\n",
        "# ê°€ì¥ ë¹ˆë²ˆí•œ ë‹¨ì–´ë“¤ í™•ì¸\n",
        "print(\"\\\\nğŸ“Š ê°€ì¥ ë¹ˆë²ˆí•œ 20ê°œ ë‹¨ì–´:\")\n",
        "most_common = fdist.most_common(20)\n",
        "for i, (word, freq) in enumerate(most_common, 1):\n",
        "    print(f\"  {i:2d}. '{word}': {freq:,}íšŒ ({freq/len(data)*100:.2f}%)\")\n",
        "\n",
        "# ë¹ˆë„ ë¶„í¬ ì‹œê°í™” ì¤€ë¹„\n",
        "words, frequencies = zip(*most_common)\n",
        "print(f\"\\\\nìƒìœ„ 10ê°œ ë‹¨ì–´ê°€ ì „ì²´ì˜ {sum(frequencies[:10])/len(data)*100:.1f}%ë¥¼ ì°¨ì§€\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. ë‹¨ì–´-ì¸ë±ìŠ¤ ë§¤í•‘ ë”•ì…”ë„ˆë¦¬ ìƒì„±\n",
        "\n",
        "íš¨ìœ¨ì ì¸ ì²˜ë¦¬ë¥¼ ìœ„í•´ ë‹¨ì–´ì™€ ì¸ë±ìŠ¤ ê°„ì˜ ì–‘ë°©í–¥ ë§¤í•‘ì„ ìƒì„±í•©ë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ë‹¨ì–´-ì¸ë±ìŠ¤ ë§¤í•‘ ë”•ì…”ë„ˆë¦¬ ìƒì„±\n",
        "print(\"ë‹¨ì–´-ì¸ë±ìŠ¤ ë§¤í•‘ ë”•ì…”ë„ˆë¦¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤...\")\n",
        "word2Ind, Ind2word = get_dict(data)\n",
        "V = len(word2Ind)\n",
        "\n",
        "print(f\"ì–´íœ˜ì§‘ í¬ê¸° (V): {V:,}\")\n",
        "print(f\"word2Ind ë”•ì…”ë„ˆë¦¬ íƒ€ì…: {type(word2Ind)}\")\n",
        "print(f\"Ind2word ë”•ì…”ë„ˆë¦¬ íƒ€ì…: {type(Ind2word)}\")\n",
        "\n",
        "# ë§¤í•‘ ì˜ˆì‹œ í™•ì¸\n",
        "test_words = ['king', 'queen', 'the', 'to', 'be']\n",
        "print(\"\\\\në‹¨ì–´ â†’ ì¸ë±ìŠ¤ ë§¤í•‘ ì˜ˆì‹œ:\")\n",
        "for word in test_words:\n",
        "    if word in word2Ind:\n",
        "        idx = word2Ind[word]\n",
        "        print(f\"  '{word}' â†’ {idx}\")\n",
        "        # ì—­ë§¤í•‘ë„ í™•ì¸\n",
        "        reverse_word = Ind2word[idx]\n",
        "        print(f\"  {idx} â†’ '{reverse_word}' âœ“\")\n",
        "    else:\n",
        "        print(f\"  '{word}' â†’ ì–´íœ˜ì§‘ì— ì—†ìŒ\")\n",
        "\n",
        "# íŠ¹ì • ì¸ë±ìŠ¤ë“¤ì˜ ë‹¨ì–´ í™•ì¸\n",
        "print(\"\\\\nì¸ë±ìŠ¤ â†’ ë‹¨ì–´ ë§¤í•‘ ì˜ˆì‹œ:\")\n",
        "sample_indices = [0, 1, 100, 500, 1000]\n",
        "for idx in sample_indices:\n",
        "    if idx < V:\n",
        "        word = Ind2word[idx]\n",
        "        print(f\"  ì¸ë±ìŠ¤ {idx} â†’ '{word}'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. ì‹ ê²½ë§ ëª¨ë¸ ì´ˆê¸°í™”\n",
        "\n",
        "CBOW ëª¨ë¸ì„ ìœ„í•œ ì‹ ê²½ë§ ê°€ì¤‘ì¹˜ì™€ í¸í–¥ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def initialize_model(N, V, random_seed=1):\n",
        "    \"\"\"\n",
        "    CBOW ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ì™€ í¸í–¥ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.\n",
        "    \n",
        "    Args:\n",
        "        N: ì€ë‹‰ì¸µ(ì„ë² ë”©) ë²¡í„°ì˜ ì°¨ì›\n",
        "        V: ì–´íœ˜ì§‘ í¬ê¸°\n",
        "        random_seed: ì¬í˜„ ê°€ëŠ¥í•œ ê²°ê³¼ë¥¼ ìœ„í•œ ì‹œë“œ\n",
        "    \n",
        "    Returns:\n",
        "        W1: ì…ë ¥ì¸µ â†’ ì€ë‹‰ì¸µ ê°€ì¤‘ì¹˜ (N Ã— V)\n",
        "        W2: ì€ë‹‰ì¸µ â†’ ì¶œë ¥ì¸µ ê°€ì¤‘ì¹˜ (V Ã— N)  \n",
        "        b1: ì€ë‹‰ì¸µ í¸í–¥ (N Ã— 1)\n",
        "        b2: ì¶œë ¥ì¸µ í¸í–¥ (V Ã— 1)\n",
        "    \"\"\"\n",
        "    np.random.seed(random_seed)\n",
        "    \n",
        "    # ê°€ì¤‘ì¹˜ë¥¼ 0ê³¼ 1 ì‚¬ì´ì˜ ë‚œìˆ˜ë¡œ ì´ˆê¸°í™”\n",
        "    W1 = np.random.rand(N, V)\n",
        "    W2 = np.random.rand(V, N)\n",
        "    b1 = np.random.rand(N, 1)\n",
        "    b2 = np.random.rand(V, 1)\n",
        "    \n",
        "    return W1, W2, b1, b2\n",
        "\n",
        "# ëª¨ë¸ íŒŒë¼ë¯¸í„° ì„¤ì •\n",
        "N = 50  # ì„ë² ë”© ì°¨ì›\n",
        "print(f\"ëª¨ë¸ ì„¤ì •:\")\n",
        "print(f\"  ì„ë² ë”© ì°¨ì› (N): {N}\")\n",
        "print(f\"  ì–´íœ˜ì§‘ í¬ê¸° (V): {V:,}\")\n",
        "\n",
        "# ëª¨ë¸ ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸\n",
        "W1, W2, b1, b2 = initialize_model(N, V, random_seed=42)\n",
        "\n",
        "print(f\"\\\\nì´ˆê¸°í™”ëœ íŒŒë¼ë¯¸í„° í¬ê¸°:\")\n",
        "print(f\"  W1 (ì…ë ¥â†’ì€ë‹‰): {W1.shape}\")\n",
        "print(f\"  W2 (ì€ë‹‰â†’ì¶œë ¥): {W2.shape}\")\n",
        "print(f\"  b1 (ì€ë‹‰ì¸µ í¸í–¥): {b1.shape}\")\n",
        "print(f\"  b2 (ì¶œë ¥ì¸µ í¸í–¥): {b2.shape}\")\n",
        "\n",
        "# ì „ì²´ íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°\n",
        "total_params = W1.size + W2.size + b1.size + b2.size\n",
        "print(f\"\\\\nì „ì²´ íŒŒë¼ë¯¸í„° ìˆ˜: {total_params:,}ê°œ\")\n",
        "\n",
        "# ê°€ì¤‘ì¹˜ ê°’ ë¶„í¬ í™•ì¸\n",
        "print(f\"\\\\nW1 í†µê³„: min={W1.min():.4f}, max={W1.max():.4f}, mean={W1.mean():.4f}\")\n",
        "print(f\"W2 í†µê³„: min={W2.min():.4f}, max={W2.max():.4f}, mean={W2.mean():.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. í™œì„±í™” í•¨ìˆ˜: Softmax êµ¬í˜„\n",
        "\n",
        "ì¶œë ¥ì¸µì—ì„œ í™•ë¥  ë¶„í¬ë¥¼ ìƒì„±í•˜ê¸° ìœ„í•œ Softmax í•¨ìˆ˜ë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def softmax(z):\n",
        "    \"\"\"\n",
        "    Softmax í™œì„±í™” í•¨ìˆ˜\n",
        "    ëª¨ë“  ì¶œë ¥ì˜ í•©ì´ 1ì´ ë˜ë„ë¡ ì •ê·œí™”í•˜ì—¬ í™•ë¥  ë¶„í¬ë¥¼ ìƒì„±\n",
        "    \n",
        "    Args:\n",
        "        z: ì€ë‹‰ì¸µì˜ ì¶œë ¥ ì ìˆ˜ (V Ã— batch_size)\n",
        "    \n",
        "    Returns:\n",
        "        yhat: í™•ë¥  ë¶„í¬ (ê° ë‹¨ì–´ì— ëŒ€í•œ ì˜ˆì¸¡ í™•ë¥ )\n",
        "    \"\"\"\n",
        "    # ìˆ˜ì¹˜ì  ì•ˆì •ì„±ì„ ìœ„í•´ ìµœëŒ€ê°’ì„ ë¹¼ì¤Œ (exp ì˜¤ë²„í”Œë¡œìš° ë°©ì§€)\n",
        "    z_shifted = z - np.max(z, axis=0, keepdims=True)\n",
        "    exp_z = np.exp(z_shifted)\n",
        "    yhat = exp_z / np.sum(exp_z, axis=0, keepdims=True)\n",
        "    return yhat\n",
        "\n",
        "# Softmax í•¨ìˆ˜ í…ŒìŠ¤íŠ¸\n",
        "print(\"Softmax í•¨ìˆ˜ í…ŒìŠ¤íŠ¸:\")\n",
        "\n",
        "# ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ë²¡í„°\n",
        "test_z = np.array([[2.0], [1.0], [0.1]])\n",
        "test_probs = softmax(test_z)\n",
        "\n",
        "print(f\"ì…ë ¥ z: {test_z.flatten()}\")\n",
        "print(f\"ì¶œë ¥ í™•ë¥ : {test_probs.flatten()}\")\n",
        "print(f\"í™•ë¥  í•©: {np.sum(test_probs):.6f} (1ì— ê°€ê¹Œì›Œì•¼ í•¨)\")\n",
        "\n",
        "# ë” ë³µì¡í•œ í…ŒìŠ¤íŠ¸ (ë°°ì¹˜ ì²˜ë¦¬)\n",
        "print(\"\\\\në°°ì¹˜ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸:\")\n",
        "batch_z = np.random.randn(5, 3)  # 5ê°œ ë‹¨ì–´, 3ê°œ ìƒ˜í”Œ\n",
        "batch_probs = softmax(batch_z)\n",
        "\n",
        "print(f\"ë°°ì¹˜ ì…ë ¥ í¬ê¸°: {batch_z.shape}\")\n",
        "print(f\"ë°°ì¹˜ ì¶œë ¥ í¬ê¸°: {batch_probs.shape}\")\n",
        "print(f\"ê° ìƒ˜í”Œì˜ í™•ë¥  í•©: {np.sum(batch_probs, axis=0)}\")\n",
        "\n",
        "# ê·¹ë‹¨ì ì¸ ê°’ì—ì„œì˜ ì•ˆì •ì„± í…ŒìŠ¤íŠ¸\n",
        "print(\"\\\\nìˆ˜ì¹˜ì  ì•ˆì •ì„± í…ŒìŠ¤íŠ¸:\")\n",
        "extreme_z = np.array([[100.0], [200.0], [300.0]])\n",
        "extreme_probs = softmax(extreme_z)\n",
        "print(f\"ê·¹ë‹¨ì  ì…ë ¥: {extreme_z.flatten()}\")\n",
        "print(f\"ì•ˆì •ì  ì¶œë ¥: {extreme_probs.flatten()}\")\n",
        "print(f\"ë¬´í•œëŒ€ë‚˜ NaN ì—†ìŒ: {np.all(np.isfinite(extreme_probs))}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. ìˆœì „íŒŒ(Forward Propagation) êµ¬í˜„\n",
        "\n",
        "ì…ë ¥ì—ì„œ ì¶œë ¥ê¹Œì§€ ì‹ ê²½ë§ì„ í†µê³¼í•˜ëŠ” ìˆœì „íŒŒ ê³¼ì •ì„ êµ¬í˜„í•©ë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def forward_prop(x, W1, W2, b1, b2):\n",
        "    \"\"\"\n",
        "    CBOW ëª¨ë¸ì˜ ìˆœì „íŒŒ\n",
        "    \n",
        "    Args:\n",
        "        x: ì»¨í…ìŠ¤íŠ¸ ë‹¨ì–´ë“¤ì˜ í‰ê·  ì›í•« ë²¡í„° (V Ã— batch_size)\n",
        "        W1, W2, b1, b2: ëª¨ë¸ íŒŒë¼ë¯¸í„°ë“¤\n",
        "    \n",
        "    Returns:\n",
        "        z: ì¶œë ¥ì¸µì˜ ì ìˆ˜ ë²¡í„° (V Ã— batch_size)\n",
        "        h: ì€ë‹‰ì¸µ ë²¡í„° (í™œì„±í™” í›„, N Ã— batch_size)\n",
        "    \"\"\"\n",
        "    # 1ë‹¨ê³„: ì…ë ¥ì¸µ â†’ ì€ë‹‰ì¸µ\n",
        "    h_raw = W1 @ x + b1  # ì„ í˜• ë³€í™˜\n",
        "    h = np.maximum(0, h_raw)  # ReLU í™œì„±í™” í•¨ìˆ˜\n",
        "    \n",
        "    # 2ë‹¨ê³„: ì€ë‹‰ì¸µ â†’ ì¶œë ¥ì¸µ\n",
        "    z = W2 @ h + b2  # ì„ í˜• ë³€í™˜ (SoftmaxëŠ” ë‚˜ì¤‘ì— ì ìš©)\n",
        "    \n",
        "    return z, h\n",
        "\n",
        "# ìˆœì „íŒŒ í…ŒìŠ¤íŠ¸\n",
        "print(\"ìˆœì „íŒŒ í•¨ìˆ˜ í…ŒìŠ¤íŠ¸:\")\n",
        "\n",
        "# ê°€ì§œ ì…ë ¥ ë°ì´í„° ìƒì„± (ì›í•« ë²¡í„°)\n",
        "batch_size = 3\n",
        "x_test = np.zeros((V, batch_size))\n",
        "\n",
        "# ëª‡ ê°œ ë‹¨ì–´ì— ëŒ€í•´ ì›í•« ì¸ì½”ë”© (ì˜ˆì‹œ)\n",
        "test_word_indices = [10, 25, 50]\n",
        "for i, word_idx in enumerate(test_word_indices):\n",
        "    if word_idx < V:\n",
        "        x_test[word_idx, i] = 1.0\n",
        "\n",
        "print(f\"í…ŒìŠ¤íŠ¸ ì…ë ¥ í¬ê¸°: {x_test.shape}\")\n",
        "print(f\"ì…ë ¥ì˜ ì›í•« ê²€ì¦: {np.sum(x_test, axis=0)} (ê°ê° 1ì´ì–´ì•¼ í•¨)\")\n",
        "\n",
        "# ìˆœì „íŒŒ ì‹¤í–‰\n",
        "z_test, h_test = forward_prop(x_test, W1, W2, b1, b2)\n",
        "\n",
        "print(f\"\\\\nìˆœì „íŒŒ ê²°ê³¼:\")\n",
        "print(f\"ì€ë‹‰ì¸µ ì¶œë ¥ (h) í¬ê¸°: {h_test.shape}\")\n",
        "print(f\"ì¶œë ¥ì¸µ ì ìˆ˜ (z) í¬ê¸°: {z_test.shape}\")\n",
        "\n",
        "# ì€ë‹‰ì¸µ í†µê³„\n",
        "print(f\"\\\\nì€ë‹‰ì¸µ (h) í†µê³„:\")\n",
        "print(f\"  ìµœì†Œê°’: {h_test.min():.4f}\")\n",
        "print(f\"  ìµœëŒ€ê°’: {h_test.max():.4f}\")\n",
        "print(f\"  í‰ê· ê°’: {h_test.mean():.4f}\")\n",
        "print(f\"  ReLUë¡œ ì¸í•œ 0 ê°œìˆ˜: {np.sum(h_test == 0)}\")\n",
        "\n",
        "# ì¶œë ¥ì¸µ í†µê³„\n",
        "print(f\"\\\\nì¶œë ¥ì¸µ (z) í†µê³„:\")\n",
        "print(f\"  ìµœì†Œê°’: {z_test.min():.4f}\")\n",
        "print(f\"  ìµœëŒ€ê°’: {z_test.max():.4f}\")\n",
        "print(f\"  í‰ê· ê°’: {z_test.mean():.4f}\")\n",
        "\n",
        "# í™•ë¥ ë¡œ ë³€í™˜í•´ë³´ê¸°\n",
        "probs_test = softmax(z_test)\n",
        "print(f\"\\\\nSoftmax í›„ í™•ë¥ :\")\n",
        "print(f\"  í™•ë¥  í•©: {np.sum(probs_test, axis=0)}\")\n",
        "print(f\"  ìµœê³  í™•ë¥ ë“¤: {np.max(probs_test, axis=0)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. ë¹„ìš© í•¨ìˆ˜(Cost Function) êµ¬í˜„\n",
        "\n",
        "ëª¨ë¸ì˜ ì˜ˆì¸¡ê³¼ ì‹¤ì œ ê°’ ì‚¬ì´ì˜ ì°¨ì´ë¥¼ ì¸¡ì •í•˜ëŠ” ë¹„ìš© í•¨ìˆ˜ë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_cost(y, yhat, batch_size):\n",
        "    \"\"\"\n",
        "    Cross-entropy ë¹„ìš© í•¨ìˆ˜\n",
        "    \n",
        "    Args:\n",
        "        y: ì‹¤ì œ ë ˆì´ë¸” (ì›í•« ë²¡í„°, V Ã— batch_size)\n",
        "        yhat: ì˜ˆì¸¡ í™•ë¥  (V Ã— batch_size)\n",
        "        batch_size: ë°°ì¹˜ í¬ê¸°\n",
        "    \n",
        "    Returns:\n",
        "        cost: í‰ê·  cross-entropy ì†ì‹¤\n",
        "    \"\"\"\n",
        "    # ìˆ˜ì¹˜ì  ì•ˆì •ì„±ì„ ìœ„í•´ log(0) ë°©ì§€\n",
        "    epsilon = 1e-15\n",
        "    yhat_clipped = np.clip(yhat, epsilon, 1 - epsilon)\n",
        "    \n",
        "    # Cross-entropy ê³„ì‚°\n",
        "    logprobs = np.multiply(np.log(yhat_clipped), y) + np.multiply(\n",
        "        np.log(1 - yhat_clipped), 1 - y\n",
        "    )\n",
        "    \n",
        "    # í‰ê·  ì†ì‹¤ ê³„ì‚°\n",
        "    cost = -1 / batch_size * np.sum(logprobs)\n",
        "    cost = np.squeeze(cost)  # ìŠ¤ì¹¼ë¼ë¡œ ë³€í™˜\n",
        "    \n",
        "    return cost\n",
        "\n",
        "# ë¹„ìš© í•¨ìˆ˜ í…ŒìŠ¤íŠ¸\n",
        "print(\"ë¹„ìš© í•¨ìˆ˜ í…ŒìŠ¤íŠ¸:\")\n",
        "\n",
        "# ì™„ë²½í•œ ì˜ˆì¸¡ ì¼€ì´ìŠ¤ (cost â‰ˆ 0)\n",
        "print(\"\\\\n1. ì™„ë²½í•œ ì˜ˆì¸¡ ì¼€ì´ìŠ¤:\")\n",
        "y_perfect = np.array([[1.0, 0.0], [0.0, 1.0], [0.0, 0.0]])  # 3Ã—2\n",
        "yhat_perfect = np.array([[0.99, 0.01], [0.01, 0.99], [0.0, 0.0]])  # 3Ã—2\n",
        "cost_perfect = compute_cost(y_perfect, yhat_perfect, 2)\n",
        "print(f\"ì‹¤ì œ: {y_perfect.T}\")\n",
        "print(f\"ì˜ˆì¸¡: {yhat_perfect.T}\")\n",
        "print(f\"ë¹„ìš©: {cost_perfect:.6f} (0ì— ê°€ê¹Œì›Œì•¼ í•¨)\")\n",
        "\n",
        "# ìµœì•…ì˜ ì˜ˆì¸¡ ì¼€ì´ìŠ¤ (cost ë†’ìŒ)\n",
        "print(\"\\\\n2. ìµœì•…ì˜ ì˜ˆì¸¡ ì¼€ì´ìŠ¤:\")\n",
        "y_worst = np.array([[1.0, 0.0], [0.0, 1.0], [0.0, 0.0]])  # 3Ã—2\n",
        "yhat_worst = np.array([[0.01, 0.99], [0.99, 0.01], [0.0, 0.0]])  # 3Ã—2\n",
        "cost_worst = compute_cost(y_worst, yhat_worst, 2)\n",
        "print(f\"ì‹¤ì œ: {y_worst.T}\")\n",
        "print(f\"ì˜ˆì¸¡: {yhat_worst.T}\")\n",
        "print(f\"ë¹„ìš©: {cost_worst:.6f} (ë†’ì•„ì•¼ í•¨)\")\n",
        "\n",
        "# ëœë¤ ì˜ˆì¸¡ ì¼€ì´ìŠ¤\n",
        "print(\"\\\\n3. ëœë¤ ì˜ˆì¸¡ ì¼€ì´ìŠ¤:\")\n",
        "np.random.seed(42)\n",
        "y_random = np.array([[1.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 0.0, 0.0]])  # 3Ã—3\n",
        "yhat_random = np.random.rand(3, 3)\n",
        "yhat_random = yhat_random / np.sum(yhat_random, axis=0)  # í™•ë¥ ë¡œ ì •ê·œí™”\n",
        "cost_random = compute_cost(y_random, yhat_random, 3)\n",
        "print(f\"ì‹¤ì œ ì²« ë²ˆì§¸ ìƒ˜í”Œ: {y_random[:, 0]}\")\n",
        "print(f\"ì˜ˆì¸¡ ì²« ë²ˆì§¸ ìƒ˜í”Œ: {yhat_random[:, 0]}\")\n",
        "print(f\"ë¹„ìš©: {cost_random:.6f}\")\n",
        "\n",
        "# ì‹¤ì œ ëª¨ë¸ ì¶œë ¥ìœ¼ë¡œ í…ŒìŠ¤íŠ¸\n",
        "print(\"\\\\n4. ì‹¤ì œ ëª¨ë¸ ì¶œë ¥ìœ¼ë¡œ í…ŒìŠ¤íŠ¸:\")\n",
        "# ì•ì„œ ìƒì„±í•œ ìˆœì „íŒŒ ê²°ê³¼ ì‚¬ìš©\n",
        "y_actual = np.zeros((V, batch_size))\n",
        "target_indices = [100, 200, 300]  # ì„ì˜ì˜ íƒ€ê²Ÿ ë‹¨ì–´ë“¤\n",
        "for i, target_idx in enumerate(target_indices):\n",
        "    if target_idx < V:\n",
        "        y_actual[target_idx, i] = 1.0\n",
        "\n",
        "yhat_actual = softmax(z_test)\n",
        "cost_actual = compute_cost(y_actual, yhat_actual, batch_size)\n",
        "print(f\"ì‹¤ì œ ëª¨ë¸ ë¹„ìš©: {cost_actual:.6f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. ì—­ì „íŒŒ(Backpropagation) êµ¬í˜„\n",
        "\n",
        "ë¹„ìš©ì„ ìµœì†Œí™”í•˜ê¸° ìœ„í•´ ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ê³„ì‚°í•˜ëŠ” ì—­ì „íŒŒë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def back_prop(x, yhat, y, h, W1, W2, b1, b2, batch_size):\n",
        "    \"\"\"\n",
        "    ì—­ì „íŒŒë¥¼ í†µí•œ ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°\n",
        "    \n",
        "    Args:\n",
        "        x: ì…ë ¥ (í‰ê·  ì›í•« ë²¡í„°, V Ã— batch_size)\n",
        "        yhat: ì˜ˆì¸¡ í™•ë¥  (V Ã— batch_size)\n",
        "        y: ì‹¤ì œ ë ˆì´ë¸” (V Ã— batch_size)\n",
        "        h: ì€ë‹‰ì¸µ ì¶œë ¥ (N Ã— batch_size)\n",
        "        W1, W2, b1, b2: ëª¨ë¸ íŒŒë¼ë¯¸í„°ë“¤\n",
        "        batch_size: ë°°ì¹˜ í¬ê¸°\n",
        "    \n",
        "    Returns:\n",
        "        grad_W1, grad_W2, grad_b1, grad_b2: ê° íŒŒë¼ë¯¸í„°ì˜ ê·¸ë˜ë””ì–¸íŠ¸\n",
        "    \"\"\"\n",
        "    # ì¶œë ¥ì¸µ ì˜¤ì°¨ (V Ã— batch_size)\n",
        "    output_error = yhat - y\n",
        "    \n",
        "    # ì€ë‹‰ì¸µìœ¼ë¡œì˜ ì—­ì „íŒŒ (ReLU í™œì„±í™” ê³ ë ¤)\n",
        "    l1 = np.dot(W2.T, output_error)  # (N Ã— batch_size)\n",
        "    l1 = np.maximum(0, l1)  # ReLU ë¯¸ë¶„ (h > 0ì¸ ê³³ë§Œ í†µê³¼)\n",
        "    \n",
        "    # ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°\n",
        "    grad_W1 = np.dot(l1, x.T) / batch_size  # (N Ã— V)\n",
        "    grad_W2 = np.dot(output_error, h.T) / batch_size  # (V Ã— N)\n",
        "    grad_b1 = np.sum(l1, axis=1, keepdims=True) / batch_size  # (N Ã— 1)\n",
        "    grad_b2 = np.sum(output_error, axis=1, keepdims=True) / batch_size  # (V Ã— 1)\n",
        "    \n",
        "    return grad_W1, grad_W2, grad_b1, grad_b2\n",
        "\n",
        "# ì—­ì „íŒŒ í…ŒìŠ¤íŠ¸\n",
        "print(\"ì—­ì „íŒŒ í•¨ìˆ˜ í…ŒìŠ¤íŠ¸:\")\n",
        "\n",
        "# ì•ì„œ ìƒì„±í•œ ìˆœì „íŒŒ ê²°ê³¼ í™œìš©\n",
        "yhat_test = softmax(z_test)\n",
        "\n",
        "# ê°€ì§œ íƒ€ê²Ÿ ìƒì„±\n",
        "y_test = np.zeros((V, batch_size))\n",
        "target_words = [word2Ind.get('king', 0), word2Ind.get('queen', 1), word2Ind.get('the', 2)]\n",
        "for i, target_idx in enumerate(target_words):\n",
        "    y_test[target_idx, i] = 1.0\n",
        "\n",
        "print(f\"í…ŒìŠ¤íŠ¸ ì„¤ì •:\")\n",
        "print(f\"  ì…ë ¥ í¬ê¸°: {x_test.shape}\")\n",
        "print(f\"  ì˜ˆì¸¡ í¬ê¸°: {yhat_test.shape}\")\n",
        "print(f\"  íƒ€ê²Ÿ í¬ê¸°: {y_test.shape}\")\n",
        "print(f\"  ì€ë‹‰ì¸µ í¬ê¸°: {h_test.shape}\")\n",
        "\n",
        "# ì—­ì „íŒŒ ì‹¤í–‰\n",
        "grad_W1, grad_W2, grad_b1, grad_b2 = back_prop(\n",
        "    x_test, yhat_test, y_test, h_test, W1, W2, b1, b2, batch_size\n",
        ")\n",
        "\n",
        "print(f\"\\\\nê³„ì‚°ëœ ê·¸ë˜ë””ì–¸íŠ¸ í¬ê¸°:\")\n",
        "print(f\"  grad_W1: {grad_W1.shape}\")\n",
        "print(f\"  grad_W2: {grad_W2.shape}\")\n",
        "print(f\"  grad_b1: {grad_b1.shape}\")\n",
        "print(f\"  grad_b2: {grad_b2.shape}\")\n",
        "\n",
        "print(f\"\\\\nê·¸ë˜ë””ì–¸íŠ¸ í†µê³„:\")\n",
        "print(f\"  grad_W1: min={grad_W1.min():.6f}, max={grad_W1.max():.6f}, mean={grad_W1.mean():.6f}\")\n",
        "print(f\"  grad_W2: min={grad_W2.min():.6f}, max={grad_W2.max():.6f}, mean={grad_W2.mean():.6f}\")\n",
        "print(f\"  grad_b1: min={grad_b1.min():.6f}, max={grad_b1.max():.6f}, mean={grad_b1.mean():.6f}\")\n",
        "print(f\"  grad_b2: min={grad_b2.min():.6f}, max={grad_b2.max():.6f}, mean={grad_b2.mean():.6f}\")\n",
        "\n",
        "# ê·¸ë˜ë””ì–¸íŠ¸ê°€ 0ì´ ì•„ë‹Œì§€ í™•ì¸ (í•™ìŠµì´ ì¼ì–´ë‚˜ëŠ”ì§€ í™•ì¸)\n",
        "print(f\"\\\\nê·¸ë˜ë””ì–¸íŠ¸ 0ì´ ì•„ë‹Œ ì›ì†Œ ë¹„ìœ¨:\")\n",
        "print(f\"  grad_W1: {np.mean(grad_W1 != 0)*100:.1f}%\")\n",
        "print(f\"  grad_W2: {np.mean(grad_W2 != 0)*100:.1f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. ê²½ì‚¬ í•˜ê°•ë²• êµ¬í˜„\n",
        "\n",
        "ëª¨ë“  êµ¬ì„± ìš”ì†Œë¥¼ ê²°í•©í•˜ì—¬ ì „ì²´ í›ˆë ¨ ê³¼ì •ì„ êµ¬í˜„í•©ë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def gradient_descent(data, word2Ind, N, V, num_iters, alpha=0.03):\n",
        "    \"\"\"\n",
        "    ê²½ì‚¬ í•˜ê°•ë²•ì„ ì‚¬ìš©í•œ CBOW ëª¨ë¸ í›ˆë ¨\n",
        "    \n",
        "    Args:\n",
        "        data: ì „ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸ ë°ì´í„°\n",
        "        word2Ind: ë‹¨ì–´â†’ì¸ë±ìŠ¤ ë”•ì…”ë„ˆë¦¬\n",
        "        N: ì„ë² ë”© ì°¨ì›\n",
        "        V: ì–´íœ˜ì§‘ í¬ê¸°\n",
        "        num_iters: í›ˆë ¨ ë°˜ë³µ íšŸìˆ˜\n",
        "        alpha: í•™ìŠµë¥ \n",
        "    \n",
        "    Returns:\n",
        "        W1, W2, b1, b2: í›ˆë ¨ëœ ëª¨ë¸ íŒŒë¼ë¯¸í„°\n",
        "    \"\"\"\n",
        "    # ëª¨ë¸ íŒŒë¼ë¯¸í„° ì´ˆê¸°í™”\n",
        "    W1, W2, b1, b2 = initialize_model(N, V, random_seed=8855)\n",
        "    \n",
        "    # í›ˆë ¨ ì„¤ì •\n",
        "    batch_size = 128\n",
        "    C = 2  # ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° í¬ê¸°\n",
        "    iters = 0\n",
        "    costs = []  # ë¹„ìš© ê¸°ë¡ìš©\n",
        "    \n",
        "    print(f\"í›ˆë ¨ ì‹œì‘:\")\n",
        "    print(f\"  í•™ìŠµë¥ : {alpha}\")\n",
        "    print(f\"  ë°°ì¹˜ í¬ê¸°: {batch_size}\")\n",
        "    print(f\"  ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš°: {C}\")\n",
        "    print(f\"  ì´ ë°˜ë³µ íšŸìˆ˜: {num_iters}\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    # í›ˆë ¨ ë£¨í”„\n",
        "    for x, y in get_batches(data, word2Ind, V, C, batch_size):\n",
        "        # ìˆœì „íŒŒ\n",
        "        z, h = forward_prop(x, W1, W2, b1, b2)\n",
        "        yhat = softmax(z)\n",
        "        \n",
        "        # ë¹„ìš© ê³„ì‚°\n",
        "        cost = compute_cost(y, yhat, batch_size)\n",
        "        costs.append(cost)\n",
        "        \n",
        "        # ì£¼ê¸°ì ìœ¼ë¡œ ë¹„ìš© ì¶œë ¥\n",
        "        if (iters + 1) % 10 == 0:\n",
        "            print(f\"ë°˜ë³µ {iters+1:3d}: ë¹„ìš© = {cost:.6f}\")\n",
        "        \n",
        "        # ì—­ì „íŒŒ\n",
        "        grad_W1, grad_W2, grad_b1, grad_b2 = back_prop(\n",
        "            x, yhat, y, h, W1, W2, b1, b2, batch_size\n",
        "        )\n",
        "        \n",
        "        # íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸\n",
        "        W1 = W1 - alpha * grad_W1\n",
        "        W2 = W2 - alpha * grad_W2\n",
        "        b1 = b1 - alpha * grad_b1\n",
        "        b2 = b2 - alpha * grad_b2\n",
        "        \n",
        "        iters += 1\n",
        "        \n",
        "        # ì¢…ë£Œ ì¡°ê±´\n",
        "        if iters == num_iters:\n",
        "            break\n",
        "            \n",
        "        # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§ (ë§¤ 100íšŒë§ˆë‹¤ ê°ì†Œ)\n",
        "        if iters % 100 == 0:\n",
        "            alpha *= 0.66\n",
        "            print(f\"    â†’ í•™ìŠµë¥ ì„ {alpha:.4f}ë¡œ ì¡°ì •\")\n",
        "    \n",
        "    print(\"-\" * 50)\n",
        "    print(f\"í›ˆë ¨ ì™„ë£Œ! ì´ {iters}íšŒ ë°˜ë³µ\")\n",
        "    print(f\"ìµœì¢… ë¹„ìš©: {costs[-1]:.6f}\")\n",
        "    print(f\"ì´ˆê¸° ë¹„ìš©: {costs[0]:.6f}\")\n",
        "    print(f\"ë¹„ìš© ê°œì„ : {costs[0] - costs[-1]:.6f}\")\n",
        "    \n",
        "    return W1, W2, b1, b2, costs\n",
        "\n",
        "print(\"ê²½ì‚¬ í•˜ê°•ë²• í•¨ìˆ˜ê°€ ì •ì˜ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. ëª¨ë¸ í›ˆë ¨ ì‹¤í–‰\n",
        "\n",
        "ì‹¤ì œë¡œ CBOW ëª¨ë¸ì„ í›ˆë ¨ì‹œì¼œë³´ê² ìŠµë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# í›ˆë ¨ íŒŒë¼ë¯¸í„° ì„¤ì •\n",
        "C = 2  # ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° í¬ê¸°\n",
        "N = 50  # ì„ë² ë”© ì°¨ì›\n",
        "num_iters = 150  # ë°˜ë³µ íšŸìˆ˜ (ë¹ ë¥¸ ì‹¤í–‰ì„ ìœ„í•´ ì›ë³¸ë³´ë‹¤ ì ê²Œ)\n",
        "\n",
        "print(\"ğŸš€ CBOW ëª¨ë¸ í›ˆë ¨ì„ ì‹œì‘í•©ë‹ˆë‹¤!\")\n",
        "print(f\"ë°ì´í„° í¬ê¸°: {len(data):,} í† í°\")\n",
        "print(f\"ì–´íœ˜ì§‘ í¬ê¸°: {V:,} ë‹¨ì–´\")\n",
        "print(f\"ì„ë² ë”© ì°¨ì›: {N}\")\n",
        "print(f\"ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš°: Â±{C}\")\n",
        "\n",
        "# ë‹¨ì–´-ì¸ë±ìŠ¤ ë”•ì…”ë„ˆë¦¬ ì¬ìƒì„± (ì •í™•ì„± í™•ë³´)\n",
        "word2Ind, Ind2word = get_dict(data)\n",
        "V = len(word2Ind)\n",
        "\n",
        "print(f\"\\\\ní›ˆë ¨ ì‹œì‘...\")\n",
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "# ëª¨ë¸ í›ˆë ¨\n",
        "W1_trained, W2_trained, b1_trained, b2_trained, training_costs = gradient_descent(\n",
        "    data, word2Ind, N, V, num_iters\n",
        ")\n",
        "\n",
        "end_time = time.time()\n",
        "training_time = end_time - start_time\n",
        "\n",
        "print(f\"\\\\nâœ… í›ˆë ¨ ì™„ë£Œ!\")\n",
        "print(f\"í›ˆë ¨ ì‹œê°„: {training_time:.1f}ì´ˆ\")\n",
        "print(f\"ìµœì¢… íŒŒë¼ë¯¸í„° í¬ê¸°:\")\n",
        "print(f\"  W1: {W1_trained.shape}\")\n",
        "print(f\"  W2: {W2_trained.shape}\")\n",
        "print(f\"  b1: {b1_trained.shape}\")\n",
        "print(f\"  b2: {b2_trained.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. í›ˆë ¨ ê³¼ì • ì‹œê°í™”\n",
        "\n",
        "í›ˆë ¨ ì¤‘ ë¹„ìš©ì˜ ë³€í™”ë¥¼ ì‹œê°í™”í•˜ì—¬ í•™ìŠµ ê³¼ì •ì„ ë¶„ì„í•©ë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# í›ˆë ¨ ê³¼ì • ì‹œê°í™”\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# ì „ì²´ ë¹„ìš© ë³€í™”\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(training_costs, 'b-', linewidth=2, alpha=0.7)\n",
        "plt.title('ì „ì²´ í›ˆë ¨ ê³¼ì •ì˜ ë¹„ìš© ë³€í™”', fontsize=14, pad=20)\n",
        "plt.xlabel('ë°˜ë³µ íšŸìˆ˜')\n",
        "plt.ylabel('Cross-entropy ì†ì‹¤')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# ìµœê·¼ 50íšŒì˜ ë¹„ìš© ë³€í™” (ì„¸ë¶€ ê´€ì°°)\n",
        "plt.subplot(1, 2, 2)\n",
        "if len(training_costs) > 50:\n",
        "    plt.plot(training_costs[-50:], 'r-', linewidth=2, alpha=0.7)\n",
        "    plt.title('ìµœê·¼ 50íšŒ ë°˜ë³µì˜ ë¹„ìš© ë³€í™”', fontsize=14, pad=20)\n",
        "    plt.xlabel('ë°˜ë³µ íšŸìˆ˜ (ìµœê·¼ 50íšŒ)')\n",
        "else:\n",
        "    plt.plot(training_costs, 'r-', linewidth=2, alpha=0.7)\n",
        "    plt.title('ì „ì²´ ë¹„ìš© ë³€í™”', fontsize=14, pad=20)\n",
        "    plt.xlabel('ë°˜ë³µ íšŸìˆ˜')\n",
        "plt.ylabel('Cross-entropy ì†ì‹¤')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# í›ˆë ¨ í†µê³„ ì¶œë ¥\n",
        "print(f\"ğŸ“Š í›ˆë ¨ í†µê³„:\")\n",
        "print(f\"  ì´ˆê¸° ë¹„ìš©: {training_costs[0]:.6f}\")\n",
        "print(f\"  ìµœì¢… ë¹„ìš©: {training_costs[-1]:.6f}\")\n",
        "print(f\"  ë¹„ìš© ê°ì†Œ: {training_costs[0] - training_costs[-1]:.6f}\")\n",
        "print(f\"  ê°ì†Œìœ¨: {((training_costs[0] - training_costs[-1]) / training_costs[0] * 100):.2f}%\")\n",
        "\n",
        "# ë¹„ìš© ë³€í™” ì¶”ì„¸ ë¶„ì„\n",
        "if len(training_costs) > 10:\n",
        "    recent_trend = np.mean(training_costs[-10:]) - np.mean(training_costs[-20:-10])\n",
        "    if recent_trend < 0:\n",
        "        print(f\"  ìµœê·¼ ì¶”ì„¸: ğŸ‘ ê³„ì† ê°ì†Œ ì¤‘ ({recent_trend:.6f})\")\n",
        "    else:\n",
        "        print(f\"  ìµœê·¼ ì¶”ì„¸: ğŸ“ˆ ì¦ê°€ ë˜ëŠ” ì•ˆì •í™” ({recent_trend:.6f})\")\n",
        "\n",
        "print(f\"\\\\ní•™ìŠµì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! ì´ì œ ì„ë² ë”©ì„ ì¶”ì¶œí•  ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. ë‹¨ì–´ ì„ë² ë”© ì¶”ì¶œ\n",
        "\n",
        "í›ˆë ¨ëœ ëª¨ë¸ì—ì„œ ë‹¨ì–´ ì„ë² ë”©ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ë‹¨ì–´ ì„ë² ë”© ì¶”ì¶œ\n",
        "# W1ê³¼ W2ì˜ í‰ê· ì„ ì‚¬ìš© (ì¼ë°˜ì ì¸ Word2Vec ì ‘ê·¼ë²•)\n",
        "embeddings = (W1_trained.T + W2_trained) / 2.0\n",
        "\n",
        "print(f\"ì¶”ì¶œëœ ì„ë² ë”©:\")\n",
        "print(f\"  í¬ê¸°: {embeddings.shape}\")\n",
        "print(f\"  ê° ë‹¨ì–´ëŠ” {embeddings.shape[1]}ì°¨ì› ë²¡í„°ë¡œ í‘œí˜„ë©ë‹ˆë‹¤\")\n",
        "\n",
        "# íŠ¹ì • ë‹¨ì–´ë“¤ì˜ ì„ë² ë”© í™•ì¸\n",
        "test_words = [\"king\", \"queen\", \"lord\", \"man\", \"woman\", \"prince\", \"ophelia\", \"rich\", \"happy\"]\n",
        "print(f\"\\\\në¶„ì„í•  ë‹¨ì–´ë“¤: {test_words}\")\n",
        "\n",
        "# ë‹¨ì–´ë“¤ì´ ì–´íœ˜ì§‘ì— ìˆëŠ”ì§€ í™•ì¸\n",
        "available_words = []\n",
        "word_indices = []\n",
        "\n",
        "for word in test_words:\n",
        "    if word in word2Ind:\n",
        "        available_words.append(word)\n",
        "        word_indices.append(word2Ind[word])\n",
        "        print(f\"  âœ“ '{word}' â†’ ì¸ë±ìŠ¤ {word2Ind[word]}\")\n",
        "    else:\n",
        "        print(f\"  âœ— '{word}' â†’ ì–´íœ˜ì§‘ì— ì—†ìŒ\")\n",
        "\n",
        "print(f\"\\\\në¶„ì„ ê°€ëŠ¥í•œ ë‹¨ì–´ ìˆ˜: {len(available_words)}/{len(test_words)}\")\n",
        "\n",
        "if len(available_words) > 0:\n",
        "    # ì„ íƒëœ ë‹¨ì–´ë“¤ì˜ ì„ë² ë”© ì¶”ì¶œ\n",
        "    X = embeddings[word_indices, :]\n",
        "    print(f\"\\\\nì„ íƒëœ ë‹¨ì–´ë“¤ì˜ ì„ë² ë”© í–‰ë ¬ í¬ê¸°: {X.shape}\")\n",
        "    \n",
        "    # ì„ë² ë”© ë²¡í„° í†µê³„\n",
        "    print(f\"\\\\nì„ë² ë”© í†µê³„:\")\n",
        "    print(f\"  í‰ê· : {X.mean():.4f}\")\n",
        "    print(f\"  í‘œì¤€í¸ì°¨: {X.std():.4f}\")\n",
        "    print(f\"  ìµœì†Ÿê°’: {X.min():.4f}\")\n",
        "    print(f\"  ìµœëŒ“ê°’: {X.max():.4f}\")\n",
        "    \n",
        "    # ê° ë‹¨ì–´ì˜ ì„ë² ë”© ë²¡í„° ì¼ë¶€ ì¶œë ¥\n",
        "    print(f\"\\\\nê° ë‹¨ì–´ì˜ ì„ë² ë”© ë²¡í„° (ì²˜ìŒ 5ì°¨ì›):\")\n",
        "    for i, word in enumerate(available_words):\n",
        "        embedding_sample = X[i, :5]\n",
        "        print(f\"  '{word}': [{', '.join([f'{x:.3f}' for x in embedding_sample])}, ...]\")\n",
        "else:\n",
        "    print(\"âš ï¸  ë¶„ì„í•  ìˆ˜ ìˆëŠ” ë‹¨ì–´ê°€ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ë‹¨ì–´ë“¤ì„ ì‹œë„í•´ë³´ì„¸ìš”.\")\n",
        "    \n",
        "    # ì–´íœ˜ì§‘ì—ì„œ ì„ì˜ì˜ ë‹¨ì–´ë“¤ ì„ íƒ\n",
        "    available_words = list(word2Ind.keys())[:9]  # ì²˜ìŒ 9ê°œ ë‹¨ì–´\n",
        "    word_indices = [word2Ind[word] for word in available_words]\n",
        "    X = embeddings[word_indices, :]\n",
        "    print(f\"\\\\nëŒ€ì‹  ë‹¤ìŒ ë‹¨ì–´ë“¤ë¡œ ë¶„ì„í•©ë‹ˆë‹¤: {available_words}\")\n",
        "    print(f\"ì„ë² ë”© í–‰ë ¬ í¬ê¸°: {X.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 14. PCAë¥¼ í†µí•œ ì„ë² ë”© ì‹œê°í™”\n",
        "\n",
        "ê³ ì°¨ì› ì„ë² ë”©ì„ 2ì°¨ì›ìœ¼ë¡œ ì¶•ì†Œí•˜ì—¬ ì‹œê°í™”í•©ë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# PCAë¥¼ ì‚¬ìš©í•˜ì—¬ 2ì°¨ì›ìœ¼ë¡œ ì¶•ì†Œ\n",
        "print(\"PCAë¥¼ ì‚¬ìš©í•˜ì—¬ ì„ë² ë”©ì„ 2ì°¨ì›ìœ¼ë¡œ ì¶•ì†Œí•©ë‹ˆë‹¤...\")\n",
        "\n",
        "try:\n",
        "    # PCA ì ìš©\n",
        "    result = compute_pca(X, 2)\n",
        "    print(f\"PCA ê²°ê³¼ í¬ê¸°: {result.shape}\")\n",
        "    print(f\"ì¶•ì†Œëœ ì¢Œí‘œ ë²”ìœ„:\")\n",
        "    print(f\"  Xì¶•: {result[:, 0].min():.3f} ~ {result[:, 0].max():.3f}\")\n",
        "    print(f\"  Yì¶•: {result[:, 1].min():.3f} ~ {result[:, 1].max():.3f}\")\n",
        "    \n",
        "    # ì‹œê°í™”\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    plt.scatter(result[:, 0], result[:, 1], c='red', s=100, alpha=0.7, edgecolors='black')\n",
        "    \n",
        "    # ê° ì ì— ë‹¨ì–´ ë ˆì´ë¸” ì¶”ê°€\n",
        "    for i, word in enumerate(available_words):\n",
        "        plt.annotate(word, \n",
        "                    xy=(result[i, 0], result[i, 1]), \n",
        "                    xytext=(5, 5), \n",
        "                    textcoords='offset points',\n",
        "                    fontsize=12, \n",
        "                    fontweight='bold',\n",
        "                    bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.7))\n",
        "    \n",
        "    plt.title('CBOW ëª¨ë¸ë¡œ í•™ìŠµí•œ ë‹¨ì–´ ì„ë² ë”© (PCA ì‹œê°í™”)', fontsize=16, pad=20)\n",
        "    plt.xlabel('ì£¼ì„±ë¶„ 1', fontsize=14)\n",
        "    plt.ylabel('ì£¼ì„±ë¶„ 2', fontsize=14)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    # ì¶• ë²”ìœ„ë¥¼ ì¡°ì •í•˜ì—¬ ë ˆì´ë¸”ì´ ì˜ë¦¬ì§€ ì•Šë„ë¡\n",
        "    x_margin = (result[:, 0].max() - result[:, 0].min()) * 0.1\n",
        "    y_margin = (result[:, 1].max() - result[:, 1].min()) * 0.1\n",
        "    plt.xlim(result[:, 0].min() - x_margin, result[:, 0].max() + x_margin)\n",
        "    plt.ylim(result[:, 1].min() - y_margin, result[:, 1].max() + y_margin)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # ë‹¨ì–´ ê°„ ê±°ë¦¬ ë¶„ì„\n",
        "    print(\"\\\\nğŸ“ ë‹¨ì–´ ê°„ ìœ í´ë¦¬ë“œ ê±°ë¦¬ (ì›ë³¸ ì„ë² ë”© ê³µê°„):\")\n",
        "    from scipy.spatial.distance import pdist, squareform\n",
        "    \n",
        "    # ì›ë³¸ ê³ ì°¨ì› ì„ë² ë”©ì—ì„œì˜ ê±°ë¦¬\n",
        "    distances = pdist(X, metric='euclidean')\n",
        "    distance_matrix = squareform(distances)\n",
        "    \n",
        "    # ê°€ì¥ ìœ ì‚¬í•œ ë‹¨ì–´ ìŒë“¤ (ê±°ë¦¬ê°€ ê°€ê¹Œìš´)\n",
        "    upper_triangle = np.triu(distance_matrix, k=1)\n",
        "    min_dist_idx = np.unravel_index(np.argmax(upper_triangle == 0), upper_triangle.shape)\n",
        "    \n",
        "    print(\"ê°€ì¥ ê°€ê¹Œìš´ ë‹¨ì–´ ìŒë“¤:\")\n",
        "    sorted_indices = np.argsort(distances)\n",
        "    for i in range(min(3, len(sorted_indices))):\n",
        "        idx = sorted_indices[i]\n",
        "        # ê±°ë¦¬ í–‰ë ¬ì—ì„œ ì¸ë±ìŠ¤ ê³„ì‚°\n",
        "        n = len(available_words)\n",
        "        row = int((-1 + np.sqrt(1 + 8*idx)) / 2)\n",
        "        col = int(idx - row * (row + 1) / 2 + row + 1)\n",
        "        if row < len(available_words) and col < len(available_words):\n",
        "            distance = distances[idx]\n",
        "            print(f\"  {available_words[row]} â†” {available_words[col]}: {distance:.3f}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"âŒ PCA ì‹œê°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
        "    print(\"ëŒ€ì‹  ë‹¨ìˆœí•œ í†µê³„ ë¶„ì„ì„ ì œê³µí•©ë‹ˆë‹¤.\")\n",
        "    \n",
        "    # ë‹¨ì–´ë³„ ì„ë² ë”© í¬ê¸°(norm) ë¹„êµ\n",
        "    norms = np.linalg.norm(X, axis=1)\n",
        "    print(\"\\\\në‹¨ì–´ë³„ ì„ë² ë”© ë²¡í„° í¬ê¸°(L2 norm):\")\n",
        "    for i, word in enumerate(available_words):\n",
        "        print(f\"  '{word}': {norms[i]:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 15. ì„ë² ë”© í’ˆì§ˆ ë¶„ì„\n",
        "\n",
        "í•™ìŠµëœ ì„ë² ë”©ì˜ í’ˆì§ˆì„ ë‹¤ì–‘í•œ ë°©ë²•ìœ¼ë¡œ ë¶„ì„í•´ë´…ì‹œë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ì„ë² ë”© í’ˆì§ˆ ë¶„ì„\n",
        "\n",
        "print(\"ğŸ” ì„ë² ë”© í’ˆì§ˆ ë¶„ì„:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# 1. ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ë¶„ì„\n",
        "def cosine_similarity(a, b):\n",
        "    \"\"\"ë‘ ë²¡í„° ê°„ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°\"\"\"\n",
        "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
        "\n",
        "if len(available_words) >= 2:\n",
        "    print(\"\\\\n1. ë‹¨ì–´ ê°„ ì½”ì‚¬ì¸ ìœ ì‚¬ë„:\")\n",
        "    for i in range(min(3, len(available_words))):\n",
        "        for j in range(i+1, min(3, len(available_words))):\n",
        "            similarity = cosine_similarity(X[i], X[j])\n",
        "            print(f\"   '{available_words[i]}' â†” '{available_words[j]}': {similarity:.4f}\")\n",
        "\n",
        "# 2. ê°€ì¥ ìœ ì‚¬í•œ/ë‹¤ë¥¸ ë‹¨ì–´ ì°¾ê¸°\n",
        "def find_most_similar_words(target_word, embeddings, word_list, word2ind, top_k=5):\n",
        "    \"\"\"íŠ¹ì • ë‹¨ì–´ì™€ ê°€ì¥ ìœ ì‚¬í•œ ë‹¨ì–´ë“¤ ì°¾ê¸°\"\"\"\n",
        "    if target_word not in word2ind:\n",
        "        return []\n",
        "    \n",
        "    target_idx = word2ind[target_word]\n",
        "    target_embedding = embeddings[target_idx]\n",
        "    \n",
        "    similarities = []\n",
        "    for word in word_list:\n",
        "        if word != target_word:\n",
        "            word_idx = word2ind[word]\n",
        "            word_embedding = embeddings[word_idx]\n",
        "            similarity = cosine_similarity(target_embedding, word_embedding)\n",
        "            similarities.append((word, similarity))\n",
        "    \n",
        "    # ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬\n",
        "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
        "    return similarities[:top_k]\n",
        "\n",
        "print(\"\\\\n2. ë‹¨ì–´ ìœ ì‚¬ë„ ìˆœìœ„:\")\n",
        "for target_word in available_words[:3]:  # ì²˜ìŒ 3ê°œ ë‹¨ì–´ì— ëŒ€í•´\n",
        "    similar_words = find_most_similar_words(target_word, embeddings, available_words, word2Ind)\n",
        "    print(f\"   '{target_word}'ì™€ ìœ ì‚¬í•œ ë‹¨ì–´ë“¤:\")\n",
        "    for word, sim in similar_words:\n",
        "        print(f\"     {word}: {sim:.4f}\")\n",
        "\n",
        "# 3. ì„ë² ë”© ê³µê°„ì˜ ë¶„ì‚° ë¶„ì„\n",
        "print(\"\\\\n3. ì„ë² ë”© ê³µê°„ ë¶„ì„:\")\n",
        "print(f\"   ì „ì²´ ì–´íœ˜ì§‘ í¬ê¸°: {embeddings.shape[0]:,} ë‹¨ì–´\")\n",
        "print(f\"   ì„ë² ë”© ì°¨ì›: {embeddings.shape[1]}\")\n",
        "\n",
        "# ê° ì°¨ì›ì˜ ë¶„ì‚°\n",
        "dimension_variances = np.var(embeddings, axis=0)\n",
        "print(f\"   ì°¨ì›ë³„ ë¶„ì‚° - í‰ê· : {dimension_variances.mean():.4f}, í‘œì¤€í¸ì°¨: {dimension_variances.std():.4f}\")\n",
        "print(f\"   ê°€ì¥ ë¶„ì‚°ì´ í° ì°¨ì›: {np.argmax(dimension_variances)} (ë¶„ì‚°: {dimension_variances.max():.4f})\")\n",
        "print(f\"   ê°€ì¥ ë¶„ì‚°ì´ ì‘ì€ ì°¨ì›: {np.argmin(dimension_variances)} (ë¶„ì‚°: {dimension_variances.min():.4f})\")\n",
        "\n",
        "# 4. ì„ë² ë”© ë²¡í„°ë“¤ì˜ ê¸¸ì´ ë¶„í¬\n",
        "embedding_norms = np.linalg.norm(embeddings, axis=1)\n",
        "print(f\"\\\\n4. ì„ë² ë”© ë²¡í„° í¬ê¸° ë¶„í¬:\")\n",
        "print(f\"   í‰ê·  í¬ê¸°: {embedding_norms.mean():.4f}\")\n",
        "print(f\"   í‘œì¤€í¸ì°¨: {embedding_norms.std():.4f}\")\n",
        "print(f\"   ìµœì†Ÿê°’: {embedding_norms.min():.4f}\")\n",
        "print(f\"   ìµœëŒ“ê°’: {embedding_norms.max():.4f}\")\n",
        "\n",
        "# 5. í•™ìŠµì˜ íš¨ê³¼ í™•ì¸\n",
        "print(\"\\\\n5. í•™ìŠµ íš¨ê³¼ ê²€ì¦:\")\n",
        "# ì´ˆê¸° ëœë¤ ì„ë² ë”©ê³¼ ë¹„êµ\n",
        "W1_random, W2_random, _, _ = initialize_model(N, V, random_seed=42)\n",
        "random_embeddings = (W1_random.T + W2_random) / 2.0\n",
        "random_norms = np.linalg.norm(random_embeddings, axis=1)\n",
        "\n",
        "print(f\"   í•™ìŠµëœ ì„ë² ë”© í‰ê·  í¬ê¸°: {embedding_norms.mean():.4f}\")\n",
        "print(f\"   ëœë¤ ì„ë² ë”© í‰ê·  í¬ê¸°: {random_norms.mean():.4f}\")\n",
        "print(f\"   ì°¨ì´: {abs(embedding_norms.mean() - random_norms.mean()):.4f}\")\n",
        "\n",
        "# ì„ë² ë”©ì´ ì˜ë¯¸ì ìœ¼ë¡œ êµ¬ì¡°í™”ë˜ì—ˆëŠ”ì§€ í™•ì¸\n",
        "print(\"\\\\nğŸ’¡ ì„ë² ë”© í•™ìŠµì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!\")\n",
        "print(\"   - ìœ ì‚¬í•œ ë‹¨ì–´ë“¤ì´ ê°€ê¹Œìš´ ìœ„ì¹˜ì— ë°°ì¹˜ë˜ì—ˆëŠ”ì§€ í™•ì¸í•´ë³´ì„¸ìš”\")\n",
        "print(\"   - ë‹¨ì–´ ê°„ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ í†µí•´ ì˜ë¯¸ì  ê´€ê³„ë¥¼ íŒŒì•…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤\")\n",
        "print(\"   - PCA ì‹œê°í™”ë¥¼ í†µí•´ ê³ ì°¨ì› ì„ë² ë”©ì˜ 2ì°¨ì› íˆ¬ì˜ì„ ê´€ì°°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ê²°ë¡ \n",
        "\n",
        "ì´ ë…¸íŠ¸ë¶ì—ì„œ ìš°ë¦¬ëŠ”:\n",
        "\n",
        "1. **CBOW ëª¨ë¸ì˜ ì „ì²´ êµ¬ì¡°**ë¥¼ ì´í•´í•˜ê³  êµ¬í˜„í–ˆìŠµë‹ˆë‹¤\n",
        "2. **ì‹ ê²½ë§ì˜ í•µì‹¬ êµ¬ì„± ìš”ì†Œë“¤**ì„ ë‹¨ê³„ë³„ë¡œ êµ¬í˜„í–ˆìŠµë‹ˆë‹¤:\n",
        "   - ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”\n",
        "   - ìˆœì „íŒŒ (Forward Propagation)\n",
        "   - Softmax í™œì„±í™” í•¨ìˆ˜\n",
        "   - Cross-entropy ë¹„ìš© í•¨ìˆ˜\n",
        "   - ì—­ì „íŒŒ (Backpropagation)\n",
        "   - ê²½ì‚¬ í•˜ê°•ë²• ìµœì í™”\n",
        "\n",
        "3. **ì‹¤ì œ í…ìŠ¤íŠ¸ ë°ì´í„°ë¡œ ëª¨ë¸ì„ í›ˆë ¨**í•˜ì—¬ ì˜ë¯¸ìˆëŠ” ë‹¨ì–´ ì„ë² ë”©ì„ í•™ìŠµí–ˆìŠµë‹ˆë‹¤\n",
        "\n",
        "4. **í•™ìŠµëœ ì„ë² ë”©ì„ ë¶„ì„í•˜ê³  ì‹œê°í™”**í•˜ì—¬ í’ˆì§ˆì„ í‰ê°€í–ˆìŠµë‹ˆë‹¤\n",
        "\n",
        "### ì£¼ìš” í•™ìŠµ í¬ì¸íŠ¸:\n",
        "\n",
        "- **CBOWì˜ í•µì‹¬ ì•„ì´ë””ì–´**: ì£¼ë³€ ë‹¨ì–´ë“¤ë¡œ ì¤‘ì‹¬ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡\n",
        "- **ì‹ ê²½ë§ ê¸°ë°˜ ì–¸ì–´ ëª¨ë¸**: í™•ë¥ ì  ì–¸ì–´ ëª¨ë¸ì„ ì‹ ê²½ë§ìœ¼ë¡œ êµ¬í˜„\n",
        "- **ì„ë² ë”© í•™ìŠµ**: í¬ì†Œí•œ ì›í•« ë²¡í„°ë¥¼ ë°€ì§‘í•œ ì˜ë¯¸ ë²¡í„°ë¡œ ë³€í™˜  \n",
        "- **ìµœì í™” ê³¼ì •**: ê·¸ë˜ë””ì–¸íŠ¸ ê¸°ë°˜ í•™ìŠµì˜ ì‹¤ì œ ì ìš©\n",
        "- **ê³ ì°¨ì› ë°ì´í„° ì‹œê°í™”**: PCAë¥¼ í†µí•œ ì°¨ì› ì¶•ì†Œì™€ í•´ì„\n",
        "\n",
        "### CBOW vs ë‹¤ë¥¸ ë°©ë²•ë“¤:\n",
        "\n",
        "- **N-gram ëª¨ë¸**: ì´ì‚°ì  í™•ë¥  vs ì—°ì†ì  ë²¡í„° í‘œí˜„\n",
        "- **Naive Bayes**: ë‹¨ìˆœ ë¶„ë¥˜ vs ë³µí•©ì  í‘œí˜„ í•™ìŠµ\n",
        "- **ë§ˆë¥´ì½”í”„ ì²´ì¸**: ìˆœì°¨ì  ìƒì„± vs ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ì˜ˆì¸¡\n",
        "\n",
        "CBOW ëª¨ë¸ì€ í˜„ëŒ€ NLPì˜ ê¸°ì´ˆê°€ ë˜ëŠ” **Word2Vec**ì˜ í•µì‹¬ ì•„ì´ë””ì–´ë¥¼ ë³´ì—¬ì£¼ë©°, íŠ¸ëœìŠ¤í¬ë¨¸ì™€ ê°™ì€ ë” ë³µì¡í•œ ëª¨ë¸ë“¤ì˜ ì´í•´ë¥¼ ìœ„í•œ ì¤‘ìš”í•œ ë””ë”¤ëŒ ì—­í• ì„ í•©ë‹ˆë‹¤!\n",
        "\n",
        "### ë‹¤ìŒ ë‹¨ê³„:\n",
        "- Skip-gram ëª¨ë¸ê³¼ì˜ ë¹„êµ\n",
        "- ë” í° ë°ì´í„°ì…‹ìœ¼ë¡œì˜ í™•ì¥\n",
        "- ì‚¬ì „ í›ˆë ¨ëœ ì„ë² ë”© (GloVe, FastText) í™œìš©\n",
        "- íŠ¸ëœìŠ¤í¬ë¨¸ ê¸°ë°˜ ëª¨ë¸ë¡œì˜ ë°œì „\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
