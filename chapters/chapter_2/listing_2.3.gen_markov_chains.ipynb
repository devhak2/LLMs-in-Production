{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 마르코프 체인을 이용한 텍스트 생성\n",
        "\n",
        "이 노트북에서는 마르코프 체인(Markov Chain)을 구현하여 텍스트를 생성하는 방법을 배워보겠습니다.\n",
        "마르코프 체인은 현재 상태가 오직 직전 상태에만 의존한다는 가정 하에 작동하는 확률적 모델입니다.\n",
        "텍스트 생성에서는 다음 단어가 현재 단어에만 의존한다고 가정합니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. 필요한 라이브러리 import\n",
        "\n",
        "마르코프 체인 구현에 필요한 라이브러리들을 import합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "필요한 라이브러리를 성공적으로 import했습니다.\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import random\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import defaultdict, deque\n",
        "\n",
        "print(\"필요한 라이브러리를 성공적으로 import했습니다.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. MarkovChain 클래스의 기본 구조\n",
        "\n",
        "마르코프 체인 클래스를 정의하고 초기화 과정을 살펴보겠습니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "랜덤 시드를 자동으로 설정했습니다.\n",
            "초기 lookup_dict 크기: 0\n"
          ]
        }
      ],
      "source": [
        "class MarkovChain:\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        마르코프 체인 초기화\n",
        "        - lookup_dict: 단어 -> 다음 가능한 단어들 목록의 매핑\n",
        "        - _seeded: 랜덤 시드 설정 여부\n",
        "        \"\"\"\n",
        "        self.lookup_dict = defaultdict(list)\n",
        "        self._seeded = False\n",
        "        self.__seed_me()\n",
        "\n",
        "    def __seed_me(self, rand_seed=None):\n",
        "        \"\"\"\n",
        "        랜덤 시드 설정 메서드\n",
        "        - rand_seed: 특정 시드값 (None이면 현재 시간 기반으로 자동 설정)\n",
        "        \"\"\"\n",
        "        if self._seeded is not True:\n",
        "            try:\n",
        "                if rand_seed is not None:\n",
        "                    random.seed(rand_seed)\n",
        "                    print(f\"랜덤 시드를 {rand_seed}로 설정했습니다.\")\n",
        "                else:\n",
        "                    random.seed()\n",
        "                    print(\"랜덤 시드를 자동으로 설정했습니다.\")\n",
        "                self._seeded = True\n",
        "            except NotImplementedError:\n",
        "                print(\"랜덤 시드 설정에 실패했습니다.\")\n",
        "                self._seeded = False\n",
        "    \n",
        "    def add_document(self, str):\n",
        "        \"\"\"\n",
        "        문서를 마르코프 체인에 추가\n",
        "        1. 텍스트 전처리\n",
        "        2. 단어 쌍 생성\n",
        "        3. lookup_dict에 단어 쌍들 저장\n",
        "        \"\"\"\n",
        "        # 1단계: 전처리\n",
        "        preprocessed_list = self._preprocess(str)\n",
        "        \n",
        "        # 2단계: 단어 쌍 생성\n",
        "        pairs = self._MarkovChain__generate_tuple_keys(preprocessed_list)\n",
        "        \n",
        "        # 3단계: lookup_dict에 저장\n",
        "        for pair in pairs:\n",
        "            self.lookup_dict[pair[0]].append(pair[1])\n",
        "                \n",
        "    def _preprocess(self, str):\n",
        "        \"\"\"\n",
        "        텍스트 전처리 메서드\n",
        "        1. 정규표현식으로 특수문자를 공백으로 대체\n",
        "        2. 소문자로 변환\n",
        "        3. NLTK word_tokenize로 토큰화\n",
        "        \"\"\"\n",
        "        # 1단계: 특수문자 제거 (알파벳, 숫자가 아닌 문자를 공백으로 대체)\n",
        "        cleaned = re.sub(r\"\\\\W+\", \" \", str).lower()\n",
        "        \n",
        "        # 2단계: 토큰화\n",
        "        tokenized = word_tokenize(cleaned)\n",
        "        \n",
        "        return tokenized\n",
        "\n",
        "    def __generate_tuple_keys(self, data):\n",
        "        \"\"\"\n",
        "        연속된 단어 쌍을 생성하는 제너레이터\n",
        "        [단어1, 단어2, 단어3, 단어4] -> [[단어1, 단어2], [단어2, 단어3], [단어3, 단어4]]\n",
        "        \"\"\"\n",
        "        if len(data) < 1:\n",
        "            return\n",
        "        \n",
        "        for i in range(len(data) - 1):\n",
        "            yield [data[i], data[i + 1]]\n",
        "\n",
        "    def generate_text(self, max_length=50):\n",
        "        \"\"\"\n",
        "        마르코프 체인을 사용하여 텍스트 생성\n",
        "        1. 시작 단어 선택 (첫 번째 키)\n",
        "        2. 현재 단어에서 가능한 다음 단어들 중 랜덤 선택\n",
        "        3. max_length에 도달하거나 더 이상 연결할 단어가 없을 때까지 반복\n",
        "        \"\"\"\n",
        "        context = deque()  # 현재 컨텍스트를 저장하는 큐\n",
        "        output = []        # 생성된 단어들을 저장하는 리스트\n",
        "        \n",
        "        if len(self.lookup_dict) > 0:\n",
        "            # 시드 재설정 (재현 가능한 결과를 위해)\n",
        "            self._MarkovChain__seed_me(rand_seed=len(self.lookup_dict))\n",
        "            \n",
        "            # 시작 단어 선택 (lookup_dict의 첫 번째 키)\n",
        "            chain_head = [list(self.lookup_dict)[0]]\n",
        "            context.extend(chain_head)\n",
        "            \n",
        "            # max_length-1 만큼 단어 생성 (마지막에 context를 추가하므로)\n",
        "            while len(output) < (max_length - 1):\n",
        "                # 현재 단어에서 가능한 다음 단어들 가져오기\n",
        "                next_choices = self.lookup_dict[context[-1]]\n",
        "                \n",
        "                if len(next_choices) > 0:\n",
        "                    # 가능한 선택지 중에서 랜덤하게 하나 선택\n",
        "                    next_word = random.choice(next_choices)\n",
        "                    context.append(next_word)\n",
        "                    output.append(context.popleft())  # 큐의 앞쪽 원소를 출력에 추가\n",
        "                else:\n",
        "                    # 더 이상 연결할 단어가 없으면 종료\n",
        "                    break\n",
        "                    \n",
        "            # 남은 컨텍스트를 출력에 추가\n",
        "            output.extend(list(context))\n",
        "            \n",
        "        return \" \".join(output)\n",
        "\n",
        "# 클래스 인스턴스 생성 테스트\n",
        "test_chain = MarkovChain()\n",
        "print(f\"초기 lookup_dict 크기: {len(test_chain.lookup_dict)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. 랜덤 시드 설정 메서드\n",
        "\n",
        "재현 가능한 결과를 위한 랜덤 시드 설정 메서드를 구현합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "시드 설정 테스트:\n",
            "랜덤 시드를 자동으로 설정했습니다.\n",
            "MarkovChain 인스턴스가 생성되었습니다.\n",
            "lookup_dict 타입: <class 'collections.defaultdict'>\n",
            "시드 설정 완료: True\n",
            "시드 설정 상태: True\n",
            "랜덤 시드를 42로 설정했습니다.\n"
          ]
        }
      ],
      "source": [
        "# 시드 설정 테스트\n",
        "print(\"시드 설정 테스트:\")\n",
        "test_chain = MarkovChain()\n",
        "print(f\"시드 설정 상태: {test_chain._seeded}\")\n",
        "\n",
        "# 특정 시드로 재설정\n",
        "test_chain._seeded = False\n",
        "test_chain._MarkovChain__seed_me(42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. 텍스트 전처리 메서드\n",
        "\n",
        "입력 텍스트를 정리하고 토큰화하는 전처리 과정을 구현합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "랜덤 시드를 자동으로 설정했습니다.\n",
            "MarkovChain 인스턴스가 생성되었습니다.\n",
            "lookup_dict 타입: <class 'collections.defaultdict'>\n",
            "시드 설정 완료: True\n",
            "원본 텍스트: Hello, World! This is a test. How are you doing today?\n",
            "전처리 결과: ['hello', ',', 'world', '!', 'this', 'is', 'a', 'test', '.', 'how', 'are', 'you', 'doing', 'today', '?']\n",
            "토큰 개수: 15\n",
            "\n",
            "복잡한 텍스트: To be, or not to be--that is the question!\n",
            "전처리 결과: ['to', 'be', ',', 'or', 'not', 'to', 'be', '--', 'that', 'is', 'the', 'question', '!']\n"
          ]
        }
      ],
      "source": [
        "# 전처리 테스트\n",
        "test_chain = MarkovChain()\n",
        "sample_text = \"Hello, World! This is a test. How are you doing today?\"\n",
        "\n",
        "print(f\"원본 텍스트: {sample_text}\")\n",
        "processed = test_chain._preprocess(sample_text)\n",
        "print(f\"전처리 결과: {processed}\")\n",
        "print(f\"토큰 개수: {len(processed)}\")\n",
        "\n",
        "# 더 복잡한 예시\n",
        "complex_text = \"To be, or not to be--that is the question!\"\n",
        "processed_complex = test_chain._preprocess(complex_text)\n",
        "print(f\"\\n복잡한 텍스트: {complex_text}\")\n",
        "print(f\"전처리 결과: {processed_complex}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. 단어 쌍 생성 메서드\n",
        "\n",
        "연속된 단어들로부터 (현재단어, 다음단어) 쌍을 생성하는 메서드를 구현합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "랜덤 시드를 자동으로 설정했습니다.\n",
            "MarkovChain 인스턴스가 생성되었습니다.\n",
            "lookup_dict 타입: <class 'collections.defaultdict'>\n",
            "시드 설정 완료: True\n",
            "입력 단어 리스트: ['to', 'be', 'or', 'not', 'to', 'be']\n",
            "생성된 단어 쌍들:\n",
            "  1: to -> be\n",
            "  2: be -> or\n",
            "  3: or -> not\n",
            "  4: not -> to\n",
            "  5: to -> be\n",
            "\n",
            "총 5개의 단어 쌍이 생성되었습니다.\n",
            "단어 하나인 경우 생성된 쌍: 0개\n"
          ]
        }
      ],
      "source": [
        "# 단어 쌍 생성 테스트\n",
        "test_chain = MarkovChain()\n",
        "test_words = [\"to\", \"be\", \"or\", \"not\", \"to\", \"be\"]\n",
        "\n",
        "print(f\"입력 단어 리스트: {test_words}\")\n",
        "print(\"생성된 단어 쌍들:\")\n",
        "\n",
        "pairs = list(test_chain._MarkovChain__generate_tuple_keys(test_words))\n",
        "for i, pair in enumerate(pairs):\n",
        "    print(f\"  {i+1}: {pair[0]} -> {pair[1]}\")\n",
        "\n",
        "print(f\"\\n총 {len(pairs)}개의 단어 쌍이 생성되었습니다.\")\n",
        "\n",
        "# 단어 하나만 있는 경우 테스트\n",
        "single_pairs = list(test_chain._MarkovChain__generate_tuple_keys([\"hello\"]))\n",
        "print(f\"단어 하나인 경우 생성된 쌍: {len(single_pairs)}개\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. 문서 추가 메서드\n",
        "\n",
        "텍스트 문서를 마르코프 체인에 학습시키는 메서드를 구현합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "랜덤 시드를 자동으로 설정했습니다.\n",
            "MarkovChain 인스턴스가 생성되었습니다.\n",
            "lookup_dict 타입: <class 'collections.defaultdict'>\n",
            "시드 설정 완료: True\n",
            "추가할 문서 1: The cat sat on the mat. The mat was comfortable.\n",
            "문서 추가 후 lookup_dict 크기: 8\n",
            "\\nlookup_dict 내용:\n",
            "  'the' -> ['cat', 'mat', 'mat']\n",
            "  'cat' -> ['sat']\n",
            "  'sat' -> ['on']\n",
            "  'on' -> ['the']\n",
            "  'mat' -> ['.', 'was']\n",
            "  '.' -> ['the']\n",
            "  'was' -> ['comfortable']\n",
            "  'comfortable' -> ['.']\n",
            "\\n추가할 문서 2: The cat likes the mat. The cat sat there.\n",
            "문서 추가 후 lookup_dict 크기: 10\n",
            "\\n업데이트된 lookup_dict 내용:\n",
            "  'the' -> ['cat', 'mat', 'mat', 'cat', 'mat', 'cat']\n",
            "  'cat' -> ['sat', 'likes', 'sat']\n",
            "  'sat' -> ['on', 'there']\n",
            "  'on' -> ['the']\n",
            "  'mat' -> ['.', 'was', '.']\n",
            "  '.' -> ['the', 'the']\n",
            "  'was' -> ['comfortable']\n",
            "  'comfortable' -> ['.']\n",
            "  'likes' -> ['the']\n",
            "  'there' -> ['.']\n"
          ]
        }
      ],
      "source": [
        "# 문서 추가 테스트\n",
        "test_chain = MarkovChain()\n",
        "\n",
        "# 간단한 테스트 문서\n",
        "test_doc1 = \"The cat sat on the mat. The mat was comfortable.\"\n",
        "print(f\"추가할 문서 1: {test_doc1}\")\n",
        "\n",
        "test_chain.add_document(test_doc1)\n",
        "print(f\"문서 추가 후 lookup_dict 크기: {len(test_chain.lookup_dict)}\")\n",
        "\n",
        "# lookup_dict 내용 확인\n",
        "print(\"\\nlookup_dict 내용:\")\n",
        "for key, values in test_chain.lookup_dict.items():\n",
        "    print(f\"  '{key}' -> {values}\")\n",
        "\n",
        "# 두 번째 문서 추가\n",
        "test_doc2 = \"The cat likes the mat. The cat sat there.\"\n",
        "print(f\"\\n추가할 문서 2: {test_doc2}\")\n",
        "\n",
        "test_chain.add_document(test_doc2)\n",
        "print(f\"문서 추가 후 lookup_dict 크기: {len(test_chain.lookup_dict)}\")\n",
        "\n",
        "# 업데이트된 lookup_dict 내용 확인\n",
        "print(\"\\n업데이트된 lookup_dict 내용:\")\n",
        "for key, values in test_chain.lookup_dict.items():\n",
        "    print(f\"  '{key}' -> {values}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. 텍스트 생성 메서드\n",
        "\n",
        "학습된 마르코프 체인을 사용하여 새로운 텍스트를 생성하는 메서드를 구현합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "텍스트 생성 테스트:\n",
            "랜덤 시드를 자동으로 설정했습니다.\n",
            "훈련 후 lookup_dict 크기: 21\n",
            "\n",
            "길이 10로 생성된 텍스트:\n",
            "  the cat sat on the dog ran in the mat\n",
            "  (실제 단어 수: 10)\n",
            "\n",
            "길이 20로 생성된 텍스트:\n",
            "  the park was happy friends . the park . the mat . the park was soft . the mat was\n",
            "  (실제 단어 수: 20)\n",
            "\n",
            "길이 30로 생성된 텍스트:\n",
            "  the cat sat on the dog ran in the mat . the cat and the dog played together . the dog was excited . they were happy . they were\n",
            "  (실제 단어 수: 30)\n"
          ]
        }
      ],
      "source": [
        "# 텍스트 생성 테스트\n",
        "print(\"텍스트 생성 테스트:\")\n",
        "test_chain = MarkovChain()\n",
        "\n",
        "# 간단한 훈련 데이터\n",
        "training_text = \"\"\"\n",
        "The cat sat on the mat. The cat was happy. The mat was soft.\n",
        "The dog ran in the park. The dog was excited. The park was big.\n",
        "The cat and the dog played together. They were happy friends.\n",
        "\"\"\"\n",
        "\n",
        "test_chain.add_document(training_text)\n",
        "print(f\"훈련 후 lookup_dict 크기: {len(test_chain.lookup_dict)}\")\n",
        "\n",
        "# 다양한 길이로 텍스트 생성\n",
        "for length in [10, 20, 30]:\n",
        "    generated = test_chain.generate_text(max_length=length)\n",
        "    print(f\"\\n길이 {length}로 생성된 텍스트:\")\n",
        "    print(f\"  {generated}\")\n",
        "    print(f\"  (실제 단어 수: {len(generated.split())})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. 햄릿 텍스트로 실제 테스트\n",
        "\n",
        "실제 햄릿 텍스트를 사용하여 마르코프 체인을 훈련하고 텍스트를 생성해봅시다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "햄릿 텍스트를 성공적으로 로드했습니다.\n",
            "텍스트 길이: 221,777 문자\n",
            "처음 200자: The Project Gutenberg EBook of Hamlet, by William Shakespeare\n",
            "\n",
            "This eBook is for the use of anyone anywhere at no cost and with\n",
            "almost no restrictions whatsoever.  You may copy it, give it away or\n",
            "re-...\n",
            "랜덤 시드를 자동으로 설정했습니다.\n",
            "\n",
            "훈련 완료!\n",
            "학습된 단어 수: 6785\n",
            "\n",
            "일부 단어들의 다음 단어 후보들:\n",
            "  'the' -> 1460개 선택지: ['project', 'use', 'terms', 'project', 'online', 'book', 'bottom', 'footnotes', 'end', 'word']...\n",
            "  'to' -> 822개 선택지: ['unmix', 'act', 'act', 'the', 'the', 'hamlet_', 'polonius_', 'command', 'future', 'be']...\n",
            "  'and' -> 808개 선택지: ['with', 'the', 'evans', 'evans', 'admiration', 'as', 'amiable', 'the', 'most', 'feeling']...\n",
            "  'be' -> 214개 선택지: ['on', 'found', 'easily', 'selected', 'spoke', 'any', 'done', 'green', 'contracted', 'thine']...\n",
            "  'of' -> 913개 선택지: ['hamlet', 'anyone', 'the', 'this', '_hamlet_', 'each', 'each', 'hamlet', 'denmark', 'denmark_']...\n"
          ]
        }
      ],
      "source": [
        "# 햄릿 텍스트 로드 및 마르코프 체인 훈련\n",
        "try:\n",
        "    with open(\"../../data/hamlet.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "        hamlet_text = f.read()\n",
        "    print(\"햄릿 텍스트를 성공적으로 로드했습니다.\")\n",
        "    print(f\"텍스트 길이: {len(hamlet_text):,} 문자\")\n",
        "    print(f\"처음 200자: {hamlet_text[:200]}...\")\n",
        "    \n",
        "except FileNotFoundError:\n",
        "    print(\"햄릿 파일을 찾을 수 없습니다. 샘플 텍스트를 사용합니다.\")\n",
        "    hamlet_text = \"\"\"\n",
        "    To be or not to be, that is the question. Whether tis nobler in the mind to suffer\n",
        "    the slings and arrows of outrageous fortune, or to take arms against a sea of troubles\n",
        "    and by opposing end them. To die, to sleep, no more, and by a sleep to say we end\n",
        "    the heartache and the thousand natural shocks that flesh is heir to.\n",
        "    \"\"\"\n",
        "\n",
        "# 마르코프 체인 생성 및 훈련\n",
        "HMM = MarkovChain()\n",
        "HMM.add_document(hamlet_text)\n",
        "\n",
        "print(f\"\\n훈련 완료!\")\n",
        "print(f\"학습된 단어 수: {len(HMM.lookup_dict)}\")\n",
        "\n",
        "# 일부 단어의 다음 단어 후보들 확인\n",
        "sample_words = ['the', 'to', 'and', 'be', 'of']\n",
        "print(\"\\n일부 단어들의 다음 단어 후보들:\")\n",
        "for word in sample_words:\n",
        "    if word in HMM.lookup_dict:\n",
        "        next_words = HMM.lookup_dict[word]\n",
        "        print(f\"  '{word}' -> {len(next_words)}개 선택지: {next_words[:10]}{'...' if len(next_words) > 10 else ''}\")\n",
        "    else:\n",
        "        print(f\"  '{word}' -> 단어를 찾을 수 없습니다.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. 다양한 길이의 텍스트 생성\n",
        "\n",
        "훈련된 마르코프 체인으로 다양한 길이의 텍스트를 생성해봅시다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "햄릿 스타일 텍스트 생성 결과:\n",
            "============================================================\n",
            "\n",
            "1. 목표 길이: 15단어 (실제: 15단어)\n",
            "   the hilts , [ footnote iv.16 : _green_ ; you to her ? _fran._ for\n",
            "\n",
            "2. 목표 길이: 25단어 (실제: 25단어)\n",
            "   the sixteenth century afterwards . polonius , or reason and passion will not beteem_ ] the loss your heart , they say ) _king._ (\n",
            "\n",
            "3. 목표 길이: 35단어 (실제: 35단어)\n",
            "   the important foe . _dan._ we may colour [ footnote iii.91 : [ 44 ] contumacious towards . _pol._ ( r. ) that rots itself more of't ; the danger of in his wit ,\n",
            "\n",
            "4. 목표 길이: 50단어 (실제: 50단어)\n",
            "   the owner of playing upon me from the time , and the terms of my beard , he was once in this folly drowns it._ ] _laer._ ( r. ) custom which the modesty as mad : _this grave rain many others . ] _queen._ ( c. centre of our\n",
            "\n",
            "============================================================\n",
            "25단어 길이로 5번 생성 (다양성 확인):\n",
            "----------------------------------------\n",
            "랜덤 시드를 6785로 설정했습니다.\n",
            "1. the carpenter ? _ ] freeze thy mother 's daughter._ ] [ 48 ] ah , rosencrantz . [ 93 ] _i.e._ , folded .\n",
            "랜덤 시드를 6785로 설정했습니다.\n",
            "2. the carpenter ? _ ] freeze thy mother 's daughter._ ] [ 48 ] ah , rosencrantz . [ 93 ] _i.e._ , folded .\n",
            "랜덤 시드를 6785로 설정했습니다.\n",
            "3. the carpenter ? _ ] freeze thy mother 's daughter._ ] [ 48 ] ah , rosencrantz . [ 93 ] _i.e._ , folded .\n",
            "랜덤 시드를 6785로 설정했습니다.\n",
            "4. the carpenter ? _ ] freeze thy mother 's daughter._ ] [ 48 ] ah , rosencrantz . [ 93 ] _i.e._ , folded .\n",
            "랜덤 시드를 6785로 설정했습니다.\n",
            "5. the carpenter ? _ ] freeze thy mother 's daughter._ ] [ 48 ] ah , rosencrantz . [ 93 ] _i.e._ , folded .\n"
          ]
        }
      ],
      "source": [
        "# 다양한 길이의 텍스트 생성\n",
        "lengths = [15, 25, 35, 50]\n",
        "\n",
        "print(\"햄릿 스타일 텍스트 생성 결과:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for i, length in enumerate(lengths, 1):\n",
        "    generated = HMM.generate_text(max_length=length)\n",
        "    actual_length = len(generated.split())\n",
        "    \n",
        "    print(f\"\\n{i}. 목표 길이: {length}단어 (실제: {actual_length}단어)\")\n",
        "    print(f\"   {generated}\")\n",
        "\n",
        "# 여러 번 생성하여 다양성 확인\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"25단어 길이로 5번 생성 (다양성 확인):\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "for i in range(5):\n",
        "    # 시드를 다시 설정하여 다른 결과 생성\n",
        "    HMM._seeded = False\n",
        "    generated = HMM.generate_text(max_length=25)\n",
        "    print(f\"{i+1}. {generated}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. 마르코프 체인 분석\n",
        "\n",
        "학습된 마르코프 체인의 특성을 분석해보겠습니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "마르코프 체인 분석 결과:\n",
            "==================================================\n",
            "고유 단어 수: 6,785\n",
            "전체 전이(transition) 수: 47,019\n",
            "평균 전이 수 per 단어: 6.93\n",
            "\n",
            "가장 많은 선택지를 가진 단어들 (Top 10):\n",
            "   1. ',': 3442개 선택지\n",
            "   2. '.': 1696개 선택지\n",
            "   3. ']': 1600개 선택지\n",
            "   4. 'the': 1460개 선택지\n",
            "   5. '[': 1131개 선택지\n",
            "   6. 'of': 913개 선택지\n",
            "   7. ':': 882개 선택지\n",
            "   8. 'to': 822개 선택지\n",
            "   9. 'and': 808개 선택지\n",
            "  10. 'a': 661개 선택지\n",
            "\n",
            "가장 적은 선택지를 가진 단어들 (Bottom 10):\n",
            "   1. 'confirmed': 1개 선택지\n",
            "   2. 'necessarily': 1개 선택지\n",
            "   3. 'main': 1개 선택지\n",
            "   4. 'pg': 1개 선택지\n",
            "   5. 'facility': 1개 선택지\n",
            "   6. '//www.gutenberg.org': 1개 선택지\n",
            "   7. 'includes': 1개 선택지\n",
            "   8. 'produce': 1개 선택지\n",
            "   9. 'subscribe': 1개 선택지\n",
            "  10. 'newsletter': 1개 선택지\n",
            "\n",
            "선택지 수 통계:\n",
            "  최대: 3442\n",
            "  최소: 1\n",
            "  평균: 6.93\n",
            "  중간값: 1\n",
            "\n",
            "특정 단어들의 다음 단어 분석:\n",
            "  'to' -> 총 822개 전이:\n",
            "    'the': 89회 (10.8%)\n",
            "    'be': 45회 (5.5%)\n",
            "    'you': 22회 (2.7%)\n",
            "    'a': 19회 (2.3%)\n",
            "    'his': 18회 (2.2%)\n",
            "  'the' -> 총 1460개 전이:\n",
            "    'king': 34회 (2.3%)\n",
            "    'project': 31회 (2.1%)\n",
            "    'same': 16회 (1.1%)\n",
            "    'very': 15회 (1.0%)\n",
            "    'world': 13회 (0.9%)\n",
            "  'be' -> 총 214개 전이:\n",
            "    ',': 13회 (6.1%)\n",
            "    'the': 10회 (4.7%)\n",
            "    'a': 10회 (4.7%)\n",
            "    'so': 5회 (2.3%)\n",
            "    'found': 4회 (1.9%)\n",
            "  'that' -> 총 348개 전이:\n",
            "    'i': 24회 (6.9%)\n",
            "    ',': 19회 (5.5%)\n",
            "    'the': 15회 (4.3%)\n",
            "    'is': 15회 (4.3%)\n",
            "    'you': 15회 (4.3%)\n",
            "  'and' -> 총 808개 전이:\n",
            "    'the': 39회 (4.8%)\n",
            "    ',': 31회 (3.8%)\n",
            "    'you': 13회 (1.6%)\n",
            "    'that': 10회 (1.2%)\n",
            "    'my': 10회 (1.2%)\n"
          ]
        }
      ],
      "source": [
        "# 마르코프 체인 통계 분석\n",
        "print(\"마르코프 체인 분석 결과:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# 전체 통계\n",
        "total_words = len(HMM.lookup_dict)\n",
        "total_transitions = sum(len(transitions) for transitions in HMM.lookup_dict.values())\n",
        "\n",
        "print(f\"고유 단어 수: {total_words:,}\")\n",
        "print(f\"전체 전이(transition) 수: {total_transitions:,}\")\n",
        "print(f\"평균 전이 수 per 단어: {total_transitions/total_words:.2f}\")\n",
        "\n",
        "# 가장 많은 다음 단어 선택지를 가진 단어들\n",
        "word_choice_counts = [(word, len(choices)) for word, choices in HMM.lookup_dict.items()]\n",
        "word_choice_counts.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "print(\"\\n가장 많은 선택지를 가진 단어들 (Top 10):\")\n",
        "for i, (word, count) in enumerate(word_choice_counts[:10], 1):\n",
        "    print(f\"  {i:2d}. '{word}': {count}개 선택지\")\n",
        "\n",
        "# 가장 적은 선택지를 가진 단어들\n",
        "print(\"\\n가장 적은 선택지를 가진 단어들 (Bottom 10):\")\n",
        "for i, (word, count) in enumerate(word_choice_counts[-10:], 1):\n",
        "    print(f\"  {i:2d}. '{word}': {count}개 선택지\")\n",
        "\n",
        "# 선택지 수의 분포\n",
        "choice_counts = [count for _, count in word_choice_counts]\n",
        "print(f\"\\n선택지 수 통계:\")\n",
        "print(f\"  최대: {max(choice_counts)}\")\n",
        "print(f\"  최소: {min(choice_counts)}\")\n",
        "print(f\"  평균: {sum(choice_counts)/len(choice_counts):.2f}\")\n",
        "print(f\"  중간값: {sorted(choice_counts)[len(choice_counts)//2]}\")\n",
        "\n",
        "# 특정 단어들의 전이 확인\n",
        "print(\"\\n특정 단어들의 다음 단어 분석:\")\n",
        "interesting_words = ['to', 'the', 'be', 'that', 'and']\n",
        "for word in interesting_words:\n",
        "    if word in HMM.lookup_dict:\n",
        "        choices = HMM.lookup_dict[word]\n",
        "        # 각 선택지의 빈도 계산\n",
        "        from collections import Counter\n",
        "        choice_freq = Counter(choices)\n",
        "        top_choices = choice_freq.most_common(5)\n",
        "        print(f\"  '{word}' -> 총 {len(choices)}개 전이:\")\n",
        "        for next_word, freq in top_choices:\n",
        "            percentage = (freq/len(choices))*100\n",
        "            print(f\"    '{next_word}': {freq}회 ({percentage:.1f}%)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. 마르코프 체인의 한계와 개선점\n",
        "\n",
        "마르코프 체인의 장단점을 살펴보고, 실제 생성 결과를 분석해봅시다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "마르코프 체인의 장단점 분석:\n",
            "==================================================\n",
            "\n",
            "✅ 장점:\n",
            "  1. 구현이 간단하고 직관적\n",
            "  2. 학습 속도가 빠름\n",
            "  3. 메모리 효율적\n",
            "  4. 원본 텍스트의 국부적 패턴을 잘 학습\n",
            "\n",
            "❌ 단점:\n",
            "  1. 장거리 의존성(long-range dependency)를 포착할 수 없음\n",
            "  2. 문법적 일관성이 떨어질 수 있음\n",
            "  3. 의미적 연관성이 부족할 수 있음\n",
            "  4. 1차 마르코프 체인은 너무 단순함\n",
            "\n",
            "🔧 개선 방안:\n",
            "  1. 고차 마르코프 체인 (2-gram, 3-gram 등)\n",
            "  2. 평활화(Smoothing) 기법 적용\n",
            "  3. 백오프(Backoff) 전략\n",
            "  4. 가중치 기반 선택\n",
            "\n",
            "📊 생성된 텍스트 품질 분석:\n",
            "랜덤 시드를 6785로 설정했습니다.\n",
            "  1. the carpenter ? _ ] freeze thy mother 's daughter._ ] [ 48 ] ah , rosencrantz . [ 93\n",
            "랜덤 시드를 6785로 설정했습니다.\n",
            "  2. the carpenter ? _ ] freeze thy mother 's daughter._ ] [ 48 ] ah , rosencrantz . [ 93\n",
            "랜덤 시드를 6785로 설정했습니다.\n",
            "  3. the carpenter ? _ ] freeze thy mother 's daughter._ ] [ 48 ] ah , rosencrantz . [ 93\n",
            "랜덤 시드를 6785로 설정했습니다.\n",
            "  4. the carpenter ? _ ] freeze thy mother 's daughter._ ] [ 48 ] ah , rosencrantz . [ 93\n",
            "랜덤 시드를 6785로 설정했습니다.\n",
            "  5. the carpenter ? _ ] freeze thy mother 's daughter._ ] [ 48 ] ah , rosencrantz . [ 93\n",
            "\n",
            "분석 결과:\n",
            "  - 문법적 완성도: 중간 수준\n",
            "  - 의미적 일관성: 낮음\n",
            "  - 원본 텍스트와의 유사성: 높음\n",
            "  - 창의성: 제한적\n",
            "\n",
            "원본 햄릿에서 실제 문장 예시:\n",
            "  1. The Project Gutenberg EBook of Hamlet, by William Shakespeare This eBook is for ...\n",
            "  2. You may copy it, give it away or re-use it under the terms of the Project Gutenb...\n",
            "  3. gutenberg...\n"
          ]
        }
      ],
      "source": [
        "# 마르코프 체인의 특성 분석\n",
        "\n",
        "print(\"마르코프 체인의 장단점 분석:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "print(\"\\n✅ 장점:\")\n",
        "print(\"  1. 구현이 간단하고 직관적\")\n",
        "print(\"  2. 학습 속도가 빠름\")\n",
        "print(\"  3. 메모리 효율적\")\n",
        "print(\"  4. 원본 텍스트의 국부적 패턴을 잘 학습\")\n",
        "\n",
        "print(\"\\n❌ 단점:\")\n",
        "print(\"  1. 장거리 의존성(long-range dependency)를 포착할 수 없음\")\n",
        "print(\"  2. 문법적 일관성이 떨어질 수 있음\")\n",
        "print(\"  3. 의미적 연관성이 부족할 수 있음\")\n",
        "print(\"  4. 1차 마르코프 체인은 너무 단순함\")\n",
        "\n",
        "# 개선 방안 시뮬레이션: 2차 마르코프 체인 아이디어\n",
        "print(\"\\n🔧 개선 방안:\")\n",
        "print(\"  1. 고차 마르코프 체인 (2-gram, 3-gram 등)\")\n",
        "print(\"  2. 평활화(Smoothing) 기법 적용\")\n",
        "print(\"  3. 백오프(Backoff) 전략\")\n",
        "print(\"  4. 가중치 기반 선택\")\n",
        "\n",
        "# 실제 생성 텍스트의 문제점 분석\n",
        "print(\"\\n📊 생성된 텍스트 품질 분석:\")\n",
        "sample_generations = []\n",
        "for i in range(5):\n",
        "    HMM._seeded = False\n",
        "    text = HMM.generate_text(max_length=20)\n",
        "    sample_generations.append(text)\n",
        "    print(f\"  {i+1}. {text}\")\n",
        "\n",
        "print(\"\\n분석 결과:\")\n",
        "print(\"  - 문법적 완성도: 중간 수준\")\n",
        "print(\"  - 의미적 일관성: 낮음\")\n",
        "print(\"  - 원본 텍스트와의 유사성: 높음\")\n",
        "print(\"  - 창의성: 제한적\")\n",
        "\n",
        "# 원본 텍스트와의 비교\n",
        "print(\"\\n원본 햄릿에서 실제 문장 예시:\")\n",
        "if len(hamlet_text) > 1000:\n",
        "    # 원본에서 몇 개 문장 추출\n",
        "    sentences = hamlet_text.split('.')[:3]\n",
        "    for i, sentence in enumerate(sentences, 1):\n",
        "        clean_sentence = ' '.join(sentence.split())[:80]\n",
        "        if clean_sentence:\n",
        "            print(f\"  {i}. {clean_sentence}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 결론\n",
        "\n",
        "이 노트북에서 우리는:\n",
        "\n",
        "1. **마르코프 체인의 기본 개념**을 이해했습니다\n",
        "2. **텍스트 전처리와 단어 쌍 생성** 과정을 학습했습니다  \n",
        "3. **확률적 텍스트 생성** 방법을 구현했습니다\n",
        "4. **실제 문학 작품을 활용한 텍스트 생성**을 수행했습니다\n",
        "5. **마르코프 체인의 특성과 한계**를 분석했습니다\n",
        "\n",
        "마르코프 체인은 자연어 처리와 텍스트 생성의 기초적이면서도 중요한 방법입니다. 비록 현대의 트랜스포머 모델들에 비해 단순하지만, 확률적 언어 모델의 기본 개념을 이해하는 데 매우 유용합니다.\n",
        "\n",
        "### 주요 학습 포인트:\n",
        "- **마르코프 가정**: 현재 상태는 직전 상태에만 의존\n",
        "- **전이 확률**: 단어 간의 연결 빈도를 통한 확률 계산\n",
        "- **랜덤 생성**: 확률 분포에 따른 다음 단어 선택\n",
        "- **국부적 일관성**: 인접한 단어들 간의 자연스러운 연결\n",
        "\n",
        "마르코프 체인을 통해 배운 이러한 개념들은 더 복잡한 언어 모델을 이해하는 기초가 됩니다!\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
